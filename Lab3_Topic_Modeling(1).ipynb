{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAB 3. TOPIC MODELING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT NORMALIZATION\n",
    "\n",
    "Below, we'll define a number of functions that perform various text \"cleaning\" jobs. After each function is defined, you can test that function by running it on a test sentence (object called test_text). At the end of the notebook, we combine all those function into one function called normolize_corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the sentence below for testing the functions. It has punctuations signs, numbers, HTML markups, and other things to take care of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get modules ready and available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html.parser\n",
      "\u001b[31m  Could not find a version that satisfies the requirement html.parser (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for html.parser\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pattern3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/0c/def740f1aaa8c7e3a8c57779187837478f0942eb00b33d4f96246ee63143/pattern3-3.0.0.tar.gz (23.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 23.7MB 1.2MB/s ta 0:00:011   17% |█████▋                          | 4.2MB 12.4MB/s eta 0:00:02    69% |██████████████████████▎         | 16.5MB 10.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /anaconda3/lib/python3.6/site-packages (from pattern3) (4.6.0)\n",
      "Collecting cherrypy (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/0e/4e353c47789ccb50130a44e765dae55b3e85abca01ff21930533ab36afc9/CherryPy-18.1.1-py2.py3-none-any.whl (417kB)\n",
      "\u001b[K    100% |████████████████████████████████| 419kB 9.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docx (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/8e/5a01644697b03016de339ef444cfff28367f92984dc74eddaab1ed60eada/docx-0.2.4.tar.gz (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 5.8MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting feedparser (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 9.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pdfminer3k (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/87/cee0aa24f95c287020df7e3936cb51d32b34b05b430759bac15f89ea5ac2/pdfminer3k-1.3.1.tar.gz (4.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.1MB 3.6MB/s eta 0:00:01    70% |██████████████████████▋         | 2.9MB 6.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting simplejson (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/24/c35fb1c1c315fc0fffe61ea00d3f88e85469004713dab488dee4f35b0aff/simplejson-3.16.0.tar.gz (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 4.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pdfminer.six (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/6e8746e6965d1a7ea8e97253e3d79e625da5547e8f376f88de5d024bacb9/pdfminer.six-20181108-py2.py3-none-any.whl (5.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 3.0MB/s ta 0:00:011    56% |██████████████████▏             | 3.2MB 19.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: more-itertools in /anaconda3/lib/python3.6/site-packages (from cherrypy->pattern3) (4.1.0)\n",
      "Collecting cheroot>=6.2.4 (from cherrypy->pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/10/96d1db9062afd476e6d63895dd3e41ced836228f904532bc640106be20ef/cheroot-6.5.4-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 7.5MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting zc.lockfile (from cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/58/c2/d7c89bdad237b4b7837609172be3e8bf5630796c0020494a15b97ece8eb1/zc.lockfile-1.4.tar.gz\n",
      "Collecting portend>=2.1.1 (from cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/81/43/21afd5914b74d4271184ee76f4093b45aa6a580dc6627d72dfc33664c6ac/portend-2.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: lxml in /anaconda3/lib/python3.6/site-packages (from docx->pattern3) (4.2.1)\n",
      "Requirement already satisfied: Pillow>=2.0 in /anaconda3/lib/python3.6/site-packages (from docx->pattern3) (5.1.0)\n",
      "Requirement already satisfied: pytest>=2.0 in /anaconda3/lib/python3.6/site-packages (from pdfminer3k->pattern3) (3.5.1)\n",
      "Requirement already satisfied: ply>=3.4 in /anaconda3/lib/python3.6/site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: sortedcontainers in /anaconda3/lib/python3.6/site-packages (from pdfminer.six->pattern3) (1.5.10)\n",
      "Collecting pycryptodome (from pdfminer.six->pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/3d/5e1766f3a3b6474df395542c903a100da1e46447dfc98d6f723c56abbdc1/pycryptodome-3.8.1-cp36-cp36m-macosx_10_6_intel.whl (10.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.1MB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from pdfminer.six->pattern3) (1.11.0)\n",
      "Collecting backports.functools-lru-cache (from cheroot>=6.2.4->cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/03/8e/2424c0e65c4a066e28f539364deee49b6451f8fcd4f718fefa50cc3dcf48/backports.functools_lru_cache-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from zc.lockfile->cherrypy->pattern3) (39.1.0)\n",
      "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/73/22900a52243fdcb2251a10bdb7c6a75fc8d40ab59ec25c01e26823af5126/tempora-1.14-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py>=1.5.0 in /anaconda3/lib/python3.6/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (1.5.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.6/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (18.1.0)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /anaconda3/lib/python3.6/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.6.0)\n",
      "Requirement already satisfied: pytz in /anaconda3/lib/python3.6/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2018.4)\n",
      "Collecting jaraco.functools>=1.20 (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/a4/3e7366d0f5e75dcad7be88524c8cbd0f3a9fb1db243269550981740c57fe/jaraco.functools-2.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: pattern3, docx, feedparser, pdfminer3k, simplejson, zc.lockfile\n",
      "  Running setup.py bdist_wheel for pattern3 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/5c/28/a0/1c9344224e87fcdd0efa74c75baaf707ac3f95211056da5889\n",
      "  Running setup.py bdist_wheel for docx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/cc/8d/09/563edfd874a35c0c7ed129b6c4fa890efa4c26458bdec6ffc1\n",
      "  Running setup.py bdist_wheel for feedparser ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "  Running setup.py bdist_wheel for pdfminer3k ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/ca/4f/a7/cb601b4fb257d2321ac668b7c6e269176780bd0283eda855d2\n",
      "  Running setup.py bdist_wheel for simplejson ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/5d/1a/1e/0350bb3df3e74215cd91325344cc86c2c691f5306eb4d22c77\n",
      "  Running setup.py bdist_wheel for zc.lockfile ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/09/27/a8/323eeff503e981b09d45ca3a0f085eb03c8a249e87157dd23f\n",
      "Successfully built pattern3 docx feedparser pdfminer3k simplejson zc.lockfile\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: backports.functools-lru-cache, cheroot, zc.lockfile, jaraco.functools, tempora, portend, cherrypy, docx, feedparser, pdfminer3k, simplejson, pycryptodome, pdfminer.six, pattern3\n",
      "Successfully installed backports.functools-lru-cache-1.5 cheroot-6.5.4 cherrypy-18.1.1 docx-0.2.4 feedparser-5.2.1 jaraco.functools-2.0 pattern3-3.0.0 pdfminer.six-20181108 pdfminer3k-1.3.1 portend-2.3 pycryptodome-3.8.1 simplejson-3.16.0 tempora-1.14 zc.lockfile-1.4\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pyLDAvis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /anaconda3/lib/python3.6/site-packages (from pyLDAvis) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /anaconda3/lib/python3.6/site-packages (from pyLDAvis) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /anaconda3/lib/python3.6/site-packages (from pyLDAvis) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=0.17.0 in /anaconda3/lib/python3.6/site-packages (from pyLDAvis) (0.23.0)\n",
      "Collecting joblib>=0.8.4 (from pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 9.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.7.2 in /anaconda3/lib/python3.6/site-packages (from pyLDAvis) (2.10)\n",
      "Requirement already satisfied: numexpr in /anaconda3/lib/python3.6/site-packages (from pyLDAvis) (2.6.5)\n",
      "Requirement already satisfied: pytest in /anaconda3/lib/python3.6/site-packages (from pyLDAvis) (3.5.1)\n",
      "Collecting future (from pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz (829kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 5.2MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting funcy (from pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/47/a4/204fa23012e913839c2da4514b92f17da82bf5fc8c2c3d902fa3fa3c6eec/funcy-1.11-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda3/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2018.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda3/lib/python3.6/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /anaconda3/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.5.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from pytest->pyLDAvis) (39.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.6/site-packages (from pytest->pyLDAvis) (18.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /anaconda3/lib/python3.6/site-packages (from pytest->pyLDAvis) (4.1.0)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /anaconda3/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.6.0)\n",
      "Building wheels for collected packages: pyLDAvis, future\n",
      "  Running setup.py bdist_wheel for pyLDAvis ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
      "Successfully built pyLDAvis future\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: joblib, future, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.11 future-0.17.1 joblib-0.13.2 pyLDAvis-2.1.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install html.parser\n",
    "import html.parser\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "!{sys.executable} -m pip install pattern3\n",
    "import pattern3\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "!{sys.executable} -m pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for lemmatization and HTML parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "html_parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the list of word contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the list of stopwords from NLTK and amend it by adding more stopwords to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text into word tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", tokenize_text(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    " \n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", expand_contractions(test_text,contraction_mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate text tokens with Part-Of-Speach tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n"
     ]
    }
   ],
   "source": [
    "def pos_tag_text(text_tokens):\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None  \n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", pos_tag_text(tokenize_text(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize text based on Part-Of-Speach (POS) tags: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    " \n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", lemmatize_text(tokenize_text(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters, such as punctuation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text \n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", remove_special_characters(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", remove_stopwords(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all non-text characters (numbers, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n"
     ]
    }
   ],
   "source": [
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", keep_text_characters(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up HTML markups: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n"
     ]
    }
   ],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "    \n",
    "def strip_html(text):\n",
    "    html_stripper = MLStripper()\n",
    "    html_stripper.feed(text)\n",
    "    return html_stripper.get_data()\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", strip_html(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing accents from characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
     ]
    }
   ],
   "source": [
    "def normalize_accented_characters(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n",
    "    return text\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", normalize_accented_characters(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all functions together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, only_text_chars=True):\n",
    "    normalized_corpus = []  \n",
    "    for index, text in enumerate(corpus):\n",
    "        text = normalize_accented_characters(text)\n",
    "        text = html.unescape(text)\n",
    "        text = strip_html(text)\n",
    "        text = expand_contractions(text, contraction_mapping)\n",
    "        text = tokenize_text(text)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if only_text_chars:\n",
    "            text = keep_text_characters(text)\n",
    "        #text = tokenize_text(text)\n",
    "        normalized_corpus.append(text)    \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small corpus consisting of 2 test sentences and testing the normalize_corpus function on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   [\"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\", \"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\"] \n",
      "\n",
      "Processed:  ['circus dog plisse skirt jump python large foot long', 'circus dog plisse skirt jump python large foot long']\n"
     ]
    }
   ],
   "source": [
    "test_corpus = [test_text]\n",
    "test_corpus.append(test_text)\n",
    "\n",
    "print(\"Original:  \", test_corpus,\"\\n\")\n",
    "print(\"Processed: \", normalize_corpus(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TOPIC MODELING: TOY CORPUS ** \n",
    "    \n",
    "First, define a small function that would display the results of fitting a topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at the toy corpus on animals and programming we discussed last session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\"The fox jumps over the dog\", \n",
    "              \"The fox is very clever and quick\", \n",
    "              \"The dog is slow and lazy\", \n",
    "              \"The cat is smarter than the fox and the dog but it can never learn Java\", \n",
    "              \"Python is an excellent programming language\", \n",
    "              \"Java and Ruby are other programming languages\", \n",
    "              \"Python and Java are very popular programming languages\", \n",
    "              \"Python programs are smaller than Java programs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and create the \"bag-of-words\" representation of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_toy_corpus = normalize_corpus(toy_corpus)\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_toy_corpus = bow_vectorizer.fit_transform(normalized_toy_corpus)\n",
    "bow_feature_names_toy_corpus = bow_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the \"bag-of-words\" representation of your corpus (never hurts to know how you data look like!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of    cat  clever  dog  excellent  fox  java  jump  language  lazy  learn  never  \\\n",
       "0    0       0    1          0    1     0     1         0     0      0      0   \n",
       "1    0       1    0          0    1     0     0         0     0      0      0   \n",
       "2    0       0    1          0    0     0     0         0     1      0      0   \n",
       "3    1       0    1          0    1     1     0         0     0      1      1   \n",
       "4    0       0    0          1    0     0     0         1     0      0      0   \n",
       "5    0       0    0          0    0     1     0         1     0      0      0   \n",
       "6    0       0    0          0    0     1     0         1     0      0      0   \n",
       "7    0       0    0          0    0     1     0         0     0      0      0   \n",
       "\n",
       "   popular  program  programming  python  quick  ruby  slow  small  smarter  \n",
       "0        0        0            0       0      0     0     0      0        0  \n",
       "1        0        0            0       0      1     0     0      0        0  \n",
       "2        0        0            0       0      0     0     1      0        0  \n",
       "3        0        0            0       0      0     0     0      0        1  \n",
       "4        0        0            1       1      0     0     0      0        0  \n",
       "5        0        0            1       0      0     1     0      0        0  \n",
       "6        1        0            1       1      0     0     0      0        0  \n",
       "7        0        2            0       1      0     0     0      1        0  >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_table = pd.DataFrame(data = bow_toy_corpus.todense(), columns = bow_feature_names_toy_corpus)\n",
    "bow_table.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select number of topics, sparsity parameters (alpha >0 and beta >0, set them to < 1 to induce sparsity) to identify and fit the topic model (LDA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "no_topics = 2\n",
    "doc_topic_prior_toy = 0.1  # alpha > 0\n",
    "topic_word_prior_toy = 0.01 # beta > 0\n",
    "lda_toy_corpus = LatentDirichletAllocation(n_components=no_topics, \n",
    "                                           doc_topic_prior = doc_topic_prior_toy,\n",
    "                                           topic_word_prior = topic_word_prior_toy).fit(bow_toy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results (we set that 10 most frequent words are displayed for each topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "dog fox lazy clever never cat slow quick java learn\n",
      "Topic 1:\n",
      "python language java programming program excellent small popular ruby fox\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "display_topics(lda_toy_corpus, bow_feature_names_toy_corpus, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at topics: word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>clever</th>\n",
       "      <th>dog</th>\n",
       "      <th>excellent</th>\n",
       "      <th>fox</th>\n",
       "      <th>java</th>\n",
       "      <th>jump</th>\n",
       "      <th>language</th>\n",
       "      <th>lazy</th>\n",
       "      <th>learn</th>\n",
       "      <th>never</th>\n",
       "      <th>popular</th>\n",
       "      <th>program</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>quick</th>\n",
       "      <th>ruby</th>\n",
       "      <th>slow</th>\n",
       "      <th>small</th>\n",
       "      <th>smarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.075991</td>\n",
       "      <td>0.034971</td>\n",
       "      <td>0.077175</td>\n",
       "      <td>0.033885</td>\n",
       "      <td>0.080783</td>\n",
       "      <td>0.079472</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.031998</td>\n",
       "      <td>0.076874</td>\n",
       "      <td>0.076307</td>\n",
       "      <td>0.036654</td>\n",
       "      <td>0.033006</td>\n",
       "      <td>0.040757</td>\n",
       "      <td>0.035852</td>\n",
       "      <td>0.035777</td>\n",
       "      <td>0.034806</td>\n",
       "      <td>0.034996</td>\n",
       "      <td>0.032881</td>\n",
       "      <td>0.075715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031221</td>\n",
       "      <td>0.070207</td>\n",
       "      <td>0.113452</td>\n",
       "      <td>0.032231</td>\n",
       "      <td>0.108731</td>\n",
       "      <td>0.035175</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>0.032133</td>\n",
       "      <td>0.072473</td>\n",
       "      <td>0.032892</td>\n",
       "      <td>0.033424</td>\n",
       "      <td>0.032364</td>\n",
       "      <td>0.034579</td>\n",
       "      <td>0.033991</td>\n",
       "      <td>0.034186</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.031387</td>\n",
       "      <td>0.070670</td>\n",
       "      <td>0.030984</td>\n",
       "      <td>0.031489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat    clever       dog  excellent       fox      java      jump  \\\n",
       "0  0.075991  0.034971  0.077175   0.033885  0.080783  0.079472  0.032882   \n",
       "0  0.031221  0.070207  0.113452   0.032231  0.108731  0.035175  0.068909   \n",
       "\n",
       "   language      lazy     learn     never   popular   program  programming  \\\n",
       "0  0.039219  0.031998  0.076874  0.076307  0.036654  0.033006     0.040757   \n",
       "0  0.032133  0.072473  0.032892  0.033424  0.032364  0.034579     0.033991   \n",
       "\n",
       "     python     quick      ruby      slow     small   smarter  \n",
       "0  0.035852  0.035777  0.034806  0.034996  0.032881  0.075715  \n",
       "0  0.034186  0.069500  0.031387  0.070670  0.030984  0.031489  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weights = lda_toy_corpus.components_ / lda_toy_corpus.components_.sum(axis=1)[:, np.newaxis]\n",
    "topic_1_words = pd.DataFrame(data = word_weights[0], index = bow_feature_names_toy_corpus)\n",
    "topic_2_words = pd.DataFrame(data = word_weights[1], index = bow_feature_names_toy_corpus)\n",
    "# topic_3_words = pd.DataFrame(data = word_weights[2], index = bow_feature_names_toy_corpus)\n",
    "tw = pd.concat([topic_1_words,topic_2_words],axis = 1)\n",
    "tw.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1:**\n",
    "1. Try adjusting the number of topics and see if you get more satisfied with the results.\n",
    "No, it's more confusing.\n",
    "2. With 2 topics, try adjusting alpha (doc_topic_prior) and beta (topic_word_prior) and observe the changes in topic representation.\n",
    "3. Comment on your findings\n",
    "It seems that the result is more satisfying when parameters are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TOPIC MODELING: NEWS **\n",
    "\n",
    "The dataset here is the one we used for doing classification. \n",
    "The newspaper blogposts have 4 topics: atheism, religion, computer graphics and space sciene. \n",
    "Of course, we will not use this information for topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data and set up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "dataset = fetch_20newsgroups(shuffle=True, \n",
    "                             random_state=1, \n",
    "                             categories = categories, \n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "news_corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the corpus and create \"bag-of-words\" representation of the data. We'll limit the number of features to 1000. \n",
    "\n",
    "NOTE: It will take a couple of minutes to get the data ready! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "\n",
    "normalized_corpus_news = normalize_corpus(news_corpus)\n",
    "\n",
    "bow_vectorizer_news = CountVectorizer(max_features=no_features)\n",
    "bow_news_corpus = bow_vectorizer_news.fit_transform(normalized_corpus_news)\n",
    "bow_feature_names_news = bow_vectorizer_news.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set number of topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics_news = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the topic model (LDA). NOTE: It will take a couple of minutes for the estimation to finish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "lda_news = LatentDirichletAllocation(n_components=no_topics_news, \n",
    "                                     max_iter=100,\n",
    "                                    doc_topic_prior = 0.25,\n",
    "                                    topic_word_prior = 0.01).fit(bow_news_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "image file use edu program software graphic format data jpeg\n",
      "Topic 1:\n",
      "would know people jesus think god like good thing time\n",
      "Topic 2:\n",
      "god people think would believe atheist argument religion use point\n",
      "Topic 3:\n",
      "space nasa launch satellite year system orbit use earth mission\n"
     ]
    }
   ],
   "source": [
    "no_top_words_news = 10\n",
    "display_topics(lda_news, bow_feature_names_news, no_top_words_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2:**\n",
    "\n",
    "1. Try 3 and 5 topics, do results get better or worse, in your opinion? \n",
    "3 is fine and 5 is worse.\n",
    "2. Introduce sparsity parameters alpha and beta (doc_topic_prior and topic_word_prior) and see if can get better results.\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** INTERACTIVE TOPIC VISUALIZATION **\n",
    "\n",
    "Relevancy weight parameter - λ (0 ≤ λ ≤ 1): you can adjust it!\n",
    "\n",
    "* small λ highlights potentially rare, but exclusive terms for the selected topic;\n",
    "* large values of λ (near 1) highlight frequent, but not necessarily exclusive, terms for the selected topic;\n",
    "\n",
    "    Relevancy = λ log[p(term | topic)] + (1 - λ) log[p(term | topic)/p(term)]\n",
    "\n",
    "Additional information on how to use this visualization:\n",
    "* http://www.kennyshirley.com/LDAvis/\n",
    "* https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el241011122505458803128538301\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el241011122505458803128538301_data = {\"mdsDat\": {\"x\": [-78.67119598388672, -33.778663635253906, 138.1947784423828, -250.6446533203125], \"y\": [-396.2831115722656, -7.443440914154053, -224.30953979492188, -179.41705322265625], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [28.387235511993147, 25.969430912469342, 23.26661734385535, 22.376716231682167]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [1048.0, 848.0, 545.0, 415.0, 399.0, 789.0, 437.0, 406.0, 285.0, 297.0, 294.0, 534.0, 798.0, 253.0, 280.0, 253.0, 245.0, 260.0, 256.0, 243.0, 246.0, 758.0, 447.0, 442.0, 416.0, 198.0, 242.0, 201.0, 281.0, 301.0, 406.7956338212563, 160.3117161701452, 122.58243243292164, 98.47878362988276, 75.9212781345258, 72.7404640528432, 68.34362003137876, 67.17546243911104, 67.04835215379157, 64.90128109934358, 62.46988015963152, 59.30072711743683, 58.7045139865724, 58.21122238045683, 58.15675749784525, 57.17942246669325, 53.191687840880874, 52.838213148384924, 49.2497481803913, 48.65690128105143, 48.249068829542026, 47.618249246258564, 46.69237627018724, 46.584673501436185, 46.14413254667229, 46.13642163619851, 45.661498576650445, 44.97148746645501, 42.56172927436975, 41.13190148905087, 134.39503230397665, 91.82728405241956, 85.94394731205306, 121.37222653045332, 66.71980814129937, 77.01355090532722, 232.38096925860617, 82.86952345405079, 260.07815276016964, 138.30563664479754, 88.8846625708308, 722.648969468884, 191.53180422850446, 184.0835164536, 144.7877481178747, 441.64729102749317, 424.0516338776402, 120.0001281126153, 399.3672233378216, 354.9433763434459, 276.62743725241535, 317.59512034777725, 369.31083107650375, 133.28246099472045, 183.3299345856944, 244.81096122147264, 100.6464933442904, 181.16180816971098, 200.9087921547054, 243.92664423570642, 276.09490461617037, 209.21400422721564, 214.94543788534443, 181.28849651091465, 207.400920724567, 165.42204595745812, 182.89749617122192, 178.477226008979, 152.58916072431182, 153.48509231068928, 164.0881002636444, 163.85414194925292, 155.21071067837647, 545.755108138197, 297.17258243354024, 294.8069303774605, 279.9738670148616, 260.37962175153086, 256.1760063181483, 246.612551260164, 196.10738936065, 195.99611803134687, 185.3182766021982, 144.59089287421028, 144.04741553761062, 142.61302026491998, 132.64139882866198, 121.89898013906458, 113.98287455198668, 113.71342113440431, 110.79922956095042, 101.88721087770502, 92.88816051539295, 92.6243566969466, 88.58601839552279, 84.32831288397887, 83.90244485396424, 83.00359810367038, 79.70970382345867, 74.65153247558762, 74.50972426438756, 73.03299081368634, 69.81180156631324, 820.0690173819333, 419.0297427401991, 235.62835804199963, 180.3870151232283, 105.01600854798997, 176.98450908358276, 252.87212006665754, 165.69835699681119, 356.5795032575988, 196.6650650632397, 156.83356363057342, 493.1440896397113, 144.14144532524872, 190.85716801505717, 196.53449794077287, 265.88722182273494, 192.9384241396073, 175.5204332784026, 223.6841529051725, 172.5054850302702, 155.70748976866858, 191.03073373898323, 200.80941502274388, 169.15951883470115, 167.8990781921209, 399.30108375964636, 285.31792721424785, 253.63399316802028, 201.14411557075854, 181.35797332987312, 181.1621956169458, 155.09797489913387, 136.61387553865814, 415.2023390939214, 132.46899615026587, 121.76610554033616, 116.025056701438, 111.36343339640169, 109.89162343705007, 107.19306400057019, 118.81199640000428, 87.7195626901024, 87.39683266524413, 78.1267334923743, 72.20557596030416, 69.64848210217032, 67.6711796473104, 66.95623376934745, 65.63602609391945, 63.7387113410321, 63.12407103963326, 60.56710061273327, 60.55958496486309, 60.555429883318595, 60.32083490159144, 1029.0066787866047, 239.1727464567928, 127.17111238446324, 107.63347814295251, 77.46032581808431, 160.49731215964107, 120.15360010539983, 240.65863525974382, 97.45978375481826, 95.33266289354462, 105.78371017296313, 159.15961330093467, 162.97973029533088, 274.75001417266816, 138.49053685520488, 259.5567108801214, 126.46319909953482, 141.37654789520818, 181.3289791156834, 250.1287894181553, 169.79518481817865, 177.73195330706923, 140.77492918379875, 157.51446224479741, 172.3775961156415, 159.3166441274293, 135.56953363029226, 253.79089968998505, 198.85491051113962, 172.37908976639326, 122.22506521095478, 73.23541728181277, 68.08967522913004, 121.69257156868841, 65.15849775447761, 61.44971535286069, 61.24706334629684, 58.55105833219766, 48.14562650676376, 45.9250816127659, 42.476733494251526, 41.10678574410576, 245.5772979613251, 73.54439547230538, 135.14513453674047, 57.80892008240245, 90.26569551265207, 61.8867763625281, 58.72864553437593, 84.7289335191641, 39.21191511401036, 150.25479724267896, 47.54025856818421, 179.76792171569556, 42.16842698013029, 41.1280234579481, 36.345411637538376, 53.35301182423019, 122.72527950172798, 61.530748969806766, 196.69229763288482, 165.01861724602162, 222.57975031448032, 152.3022031356084, 278.9766967371464, 79.57748308095421, 90.27440975903622, 419.859179595355, 126.12640983348757, 186.65361131564515, 165.0312597877131, 168.65201696612448, 136.04951696451056, 332.3179169084744, 308.5369247419976, 139.73128167993372, 100.69480933014451, 202.2090074011836, 108.84031166978406, 153.0282787006299, 188.93326898198666, 164.39083278156963, 292.22401142679445, 166.48123323072852, 213.3014566722193, 172.46177820532267, 164.0580479177347, 160.90897863908003, 156.41171787144802, 147.98799448043613, 160.09015623337348, 154.4871289074862], \"Term\": [\"space\", \"image\", \"file\", \"nasa\", \"launch\", \"god\", \"edu\", \"jesus\", \"satellite\", \"software\", \"graphic\", \"program\", \"people\", \"atheist\", \"format\", \"orbit\", \"argument\", \"jpeg\", \"ftp\", \"mission\", \"color\", \"think\", \"data\", \"year\", \"believe\", \"atheism\", \"mail\", \"shuttle\", \"earth\", \"religion\", \"jesus\", \"child\", \"matthew\", \"christ\", \"koresh\", \"verse\", \"jew\", \"quran\", \"mormon\", \"greek\", \"lord\", \"juda\", \"prophecy\", \"prophet\", \"tyre\", \"king\", \"gospel\", \"father\", \"shall\", \"jewish\", \"biblical\", \"hang\", \"bobby\", \"allah\", \"holy\", \"heaven\", \"satan\", \"lds\", \"testament\", \"follower\", \"love\", \"ra\", \"men\", \"die\", \"saw\", \"passage\", \"bible\", \"story\", \"christian\", \"man\", \"speak\", \"would\", \"day\", \"really\", \"never\", \"know\", \"people\", \"live\", \"think\", \"like\", \"thing\", \"good\", \"god\", \"sure\", \"life\", \"even\", \"maybe\", \"book\", \"us\", \"could\", \"time\", \"give\", \"take\", \"want\", \"well\", \"seem\", \"way\", \"look\", \"world\", \"right\", \"year\", \"many\", \"mean\", \"file\", \"software\", \"graphic\", \"format\", \"jpeg\", \"ftp\", \"color\", \"gif\", \"pub\", \"display\", \"graphics\", \"3d\", \"email\", \"info\", \"directory\", \"pc\", \"mac\", \"server\", \"video\", \"processing\", \"screen\", \"unix\", \"algorithm\", \"hi\", \"driver\", \"pixel\", \"sgi\", \"comp\", \"amiga\", \"viewer\", \"image\", \"edu\", \"mail\", \"package\", \"window\", \"thanks\", \"available\", \"computer\", \"program\", \"version\", \"code\", \"use\", \"user\", \"send\", \"line\", \"data\", \"bit\", \"information\", \"system\", \"please\", \"run\", \"know\", \"would\", \"work\", \"look\", \"launch\", \"satellite\", \"orbit\", \"shuttle\", \"lunar\", \"moon\", \"rocket\", \"flight\", \"nasa\", \"station\", \"probe\", \"spacecraft\", \"dc\", \"solar\", \"vehicle\", \"development\", \"planetary\", \"fund\", \"orbital\", \"payload\", \"telescope\", \"russian\", \"washington\", \"orbiter\", \"propulsion\", \"astronaut\", \"km\", \"institute\", \"venus\", \"energy\", \"space\", \"mission\", \"national\", \"mar\", \"office\", \"technology\", \"report\", \"earth\", \"star\", \"sci\", \"service\", \"cost\", \"center\", \"year\", \"research\", \"system\", \"design\", \"science\", \"data\", \"use\", \"first\", \"program\", \"high\", \"new\", \"time\", \"would\", \"include\", \"atheist\", \"atheism\", \"moral\", \"morality\", \"gay\", \"fallacy\", \"islam\", \"argue\", \"sex\", \"homosexual\", \"premise\", \"homosexuality\", \"murder\", \"behavior\", \"consequence\", \"argument\", \"definition\", \"statement\", \"logic\", \"existence\", \"islamic\", \"anti\", \"muslim\", \"imply\", \"religious\", \"majority\", \"evidence\", \"assumption\", \"believer\", \"secular\", \"animal\", \"objective\", \"false\", \"exist\", \"belief\", \"religion\", \"human\", \"believe\", \"prove\", \"conclusion\", \"god\", \"person\", \"must\", \"example\", \"true\", \"value\", \"people\", \"think\", \"claim\", \"accept\", \"point\", \"cause\", \"something\", \"many\", \"mean\", \"would\", \"way\", \"use\", \"well\", \"take\", \"even\", \"may\", \"question\", \"know\", \"system\"], \"Total\": [1048.0, 848.0, 545.0, 415.0, 399.0, 789.0, 437.0, 406.0, 285.0, 297.0, 294.0, 534.0, 798.0, 253.0, 280.0, 253.0, 245.0, 260.0, 256.0, 243.0, 246.0, 758.0, 447.0, 442.0, 416.0, 198.0, 242.0, 201.0, 281.0, 301.0, 406.82572612476736, 160.3418086756224, 122.61252857974937, 98.50887592754023, 75.95137044864676, 72.77055634682571, 68.37371233592782, 67.20555487699615, 67.0784445001703, 64.93137339448265, 62.4999724608889, 59.33081941043727, 58.73460628112049, 58.24131467423649, 58.18684988813318, 57.20951479967111, 53.221780140884356, 52.8683054415959, 49.27984047461389, 48.686993574738295, 48.279161128176845, 47.64834154263998, 46.72246856324799, 46.61476579423969, 46.174224839973014, 46.16651392949307, 45.691590870183695, 45.001579800428594, 42.59182156761829, 41.16199383498125, 135.4915992877124, 93.05617293820167, 87.07113737719878, 125.98718826526944, 68.0271343894995, 80.45482641578853, 287.08303885065914, 91.43060242691686, 368.81868698594315, 175.09133997357984, 102.00898197343233, 1374.9990400458516, 267.82097334377073, 255.6008510261534, 188.84433111243757, 825.5311087150887, 798.2137079136988, 150.94539105757465, 758.9059220282693, 695.883144214558, 492.4367517792591, 604.7283465452782, 789.1900757595267, 182.45484499549252, 295.58898188375133, 458.8527787928162, 121.87667867801191, 306.0124549549355, 365.97547976860864, 526.1891275146379, 656.4710842861534, 432.11086580804215, 486.7075611582004, 379.88900388353636, 523.5186153827891, 325.19802218261, 438.8400712589347, 425.6107439988711, 274.93384717610843, 293.4378214325827, 442.66121683642484, 479.4162716431181, 365.9946769623567, 545.7852813212888, 297.2027555602908, 294.83710352795026, 280.0040401592723, 260.40979488495736, 256.2061808430003, 246.6427244103778, 196.137562523832, 196.02629252091432, 185.34844972830845, 144.62106600664188, 144.07758868082286, 142.64319339469526, 132.6715719549047, 121.92915326773404, 114.01304767761916, 113.74359426483122, 110.82940268684628, 101.9173840032156, 92.9183336409598, 92.65452982528242, 88.61619152186631, 84.35848614491783, 83.93261800825579, 83.03377123828646, 79.73987695056441, 74.68170560111524, 74.53989738990447, 73.06316394053647, 69.84197469175726, 848.5722985934538, 437.5239891575037, 242.76364411961245, 184.54920463650427, 106.24171933190435, 186.54738470106574, 315.31044442748436, 192.5549476530368, 534.3315940845913, 248.85574033462302, 191.43028901196118, 1096.141168191384, 171.4521219815056, 264.0508272538576, 276.3630192257827, 447.23633845852345, 284.4002098412169, 295.569456041365, 637.7381031609068, 293.8147484217643, 228.13759620103212, 825.5311087150887, 1374.9990400458516, 419.0357136776242, 425.6107439988711, 399.3312507752789, 285.3480946479572, 253.66416018328192, 201.17428259851155, 181.38814033898188, 181.1923626224662, 155.12814189770646, 136.64404254705093, 415.2944583435051, 132.49916314730504, 121.79627256023491, 116.05522370378995, 111.3936003944308, 109.92179043869643, 107.22323099941158, 118.84832854781287, 87.74972978578629, 87.42699966608113, 78.15690049097654, 72.2357429639417, 69.67865031695769, 67.70134919138548, 66.98640076684336, 65.66619309101216, 63.768878338102866, 63.154238036481715, 60.59726761296993, 60.5897522430276, 60.58559688068558, 60.35100213792916, 1048.1096638441686, 243.06731309850846, 129.97984415699884, 109.99476734670004, 78.15376219341198, 174.04352398921827, 128.38353276578388, 281.1577929014494, 103.22296230882142, 100.74996890253303, 116.5002125337389, 200.14329281784418, 213.45812416681605, 442.66121683642484, 193.0850921111771, 637.7381031609068, 177.23963188216788, 239.99501577311247, 447.23633845852345, 1096.141168191384, 412.16418504561005, 534.3315940845913, 247.3952130101608, 379.12969998677636, 656.4710842861534, 1374.9990400458516, 384.29398003460045, 253.8216066102321, 198.88560131334896, 172.4092656589692, 122.25524062877417, 73.26559267697691, 68.11985062494219, 121.74788980741653, 65.18867341876187, 61.47989074929763, 61.27723874141886, 58.58123372730509, 48.1758019015232, 45.95525708180027, 42.50690889346503, 41.13696198906538, 245.78245564025858, 74.59803682932298, 137.53644545781063, 59.069525540993446, 92.43186676323948, 63.6607944042315, 60.75167404429412, 88.04350461545776, 40.85481021335622, 156.66730743376036, 49.92869000014141, 195.07796636810437, 45.78740983882189, 44.87911217872746, 39.71243340675815, 58.33038753638777, 136.67191447844544, 68.94179911552533, 244.07883126193482, 206.9580676483086, 301.7293761164437, 194.9622098259059, 416.7946625266265, 93.0067179879658, 109.20496195984188, 789.1900757595267, 169.50258329899373, 292.8044253722675, 265.83101860060435, 282.5152781967912, 206.17437486486807, 798.2137079136988, 758.9059220282693, 227.1540663809525, 133.8850074863122, 494.4507198834295, 155.79175443428778, 300.86211169844944, 479.4162716431181, 365.9946769623567, 1374.9990400458516, 438.8400712589347, 1096.141168191384, 523.5186153827891, 486.7075611582004, 458.8527787928162, 469.63967587908326, 338.89120393801545, 825.5311087150887, 637.7381031609068], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2592, 1.259, 1.259, 1.2589, 1.2588, 1.2588, 1.2588, 1.2588, 1.2588, 1.2588, 1.2587, 1.2587, 1.2587, 1.2587, 1.2587, 1.2587, 1.2587, 1.2587, 1.2586, 1.2586, 1.2586, 1.2586, 1.2586, 1.2586, 1.2586, 1.2586, 1.2586, 1.2586, 1.2585, 1.2585, 1.2511, 1.2459, 1.2462, 1.2219, 1.2398, 1.2155, 1.0478, 1.1609, 0.9099, 1.0234, 1.1215, 0.6159, 0.924, 0.931, 0.9936, 0.6337, 0.6267, 1.0298, 0.6172, 0.586, 0.6825, 0.6152, 0.4999, 0.9452, 0.7815, 0.631, 1.0678, 0.735, 0.6595, 0.4904, 0.3931, 0.5339, 0.442, 0.5194, 0.3333, 0.5833, 0.384, 0.3902, 0.6704, 0.6112, 0.2668, 0.1856, 0.4014, 1.3482, 1.3481, 1.3481, 1.3481, 1.3481, 1.3481, 1.3481, 1.3481, 1.3481, 1.3481, 1.348, 1.348, 1.348, 1.348, 1.348, 1.348, 1.348, 1.348, 1.348, 1.3479, 1.3479, 1.3479, 1.3479, 1.3479, 1.3479, 1.3479, 1.3478, 1.3478, 1.3478, 1.3478, 1.3141, 1.3051, 1.3184, 1.3254, 1.3366, 1.2956, 1.1276, 1.198, 0.9438, 1.1129, 1.1489, 0.5495, 1.1747, 1.0236, 1.0074, 0.8282, 0.9602, 0.8271, 0.3006, 0.8157, 0.9663, -0.1153, -0.5756, 0.4411, 0.4181, 1.4581, 1.458, 1.458, 1.458, 1.458, 1.458, 1.458, 1.4579, 1.4579, 1.4579, 1.4579, 1.4579, 1.4579, 1.4579, 1.4579, 1.4578, 1.4578, 1.4578, 1.4578, 1.4577, 1.4577, 1.4577, 1.4577, 1.4577, 1.4577, 1.4577, 1.4577, 1.4577, 1.4577, 1.4577, 1.4398, 1.442, 1.4363, 1.4364, 1.4492, 1.3771, 1.3919, 1.3026, 1.4007, 1.4029, 1.3617, 1.229, 1.1883, 0.9812, 1.1258, 0.5592, 1.1206, 0.929, 0.5554, -0.0194, 0.5713, 0.3574, 0.8943, 0.5798, 0.121, -0.6972, 0.4162, 1.497, 1.497, 1.497, 1.4969, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4966, 1.4965, 1.4965, 1.4964, 1.4964, 1.4963, 1.4829, 1.4796, 1.4756, 1.4734, 1.4689, 1.4633, 1.4588, 1.4561, 1.4554, 1.4481, 1.4154, 1.4148, 1.4099, 1.4086, 1.408, 1.3895, 1.3834, 1.2813, 1.2707, 1.1929, 1.2502, 1.0957, 1.3412, 1.3068, 0.8661, 1.2016, 1.0469, 1.0204, 0.9813, 1.0814, 0.6209, 0.5971, 1.0112, 1.2123, 0.603, 1.1385, 0.8211, 0.566, 0.6968, -0.0515, 0.5279, -0.1397, 0.3868, 0.4097, 0.4493, 0.3977, 0.6686, -0.1431, 0.0793], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.4106, -5.3418, -5.6101, -5.829, -6.0892, -6.132, -6.1943, -6.2116, -6.2135, -6.246, -6.2842, -6.3362, -6.3464, -6.3548, -6.3557, -6.3727, -6.445, -6.4516, -6.522, -6.5341, -6.5425, -6.5557, -6.5753, -6.5776, -6.5871, -6.5873, -6.5976, -6.6128, -6.6679, -6.7021, -5.5181, -5.899, -5.9652, -5.62, -6.2184, -6.0749, -4.9705, -6.0016, -4.8579, -5.4894, -5.9315, -3.8359, -5.1638, -5.2035, -5.4436, -4.3284, -4.369, -5.6314, -4.429, -4.5469, -4.7962, -4.6581, -4.5072, -5.5264, -5.2076, -4.9184, -5.8073, -5.2195, -5.116, -4.922, -4.7981, -5.0755, -5.0485, -5.2188, -5.0842, -5.3104, -5.2099, -5.2344, -5.3911, -5.3853, -5.3185, -5.3199, -5.3741, -4.0277, -4.6355, -4.6435, -4.6952, -4.7677, -4.784, -4.822, -5.0512, -5.0518, -5.1078, -5.3559, -5.3597, -5.3697, -5.4422, -5.5267, -5.5938, -5.5962, -5.6221, -5.706, -5.7985, -5.8013, -5.8459, -5.8951, -5.9002, -5.911, -5.9515, -6.017, -6.0189, -6.0389, -6.084, -3.6205, -4.2919, -4.8676, -5.1347, -5.6757, -5.1538, -4.797, -5.2197, -4.4533, -5.0483, -5.2747, -4.129, -5.3591, -5.0783, -5.049, -4.7468, -5.0675, -5.1621, -4.9196, -5.1794, -5.2819, -5.0774, -5.0275, -5.199, -5.2065, -4.2302, -4.5663, -4.6841, -4.9159, -5.0195, -5.0206, -5.1759, -5.3028, -4.1912, -5.3336, -5.4178, -5.4661, -5.5072, -5.5205, -5.5453, -5.4424, -5.7458, -5.7495, -5.8616, -5.9404, -5.9765, -6.0053, -6.0159, -6.0358, -6.0652, -6.0748, -6.1162, -6.1163, -6.1164, -6.1203, -3.2836, -4.7428, -5.3744, -5.5412, -5.8702, -5.1417, -5.4312, -4.7366, -5.6405, -5.6626, -5.5586, -5.15, -5.1263, -4.6041, -5.2891, -4.661, -5.38, -5.2685, -5.0196, -4.698, -5.0854, -5.0397, -5.2728, -5.1604, -5.0703, -5.1491, -5.3105, -4.6444, -4.8884, -5.0313, -5.3751, -5.8873, -5.9601, -5.3795, -6.0041, -6.0627, -6.066, -6.1111, -6.3067, -6.3539, -6.432, -6.4648, -4.6773, -5.8831, -5.2746, -6.1238, -5.6782, -6.0556, -6.108, -5.7415, -6.512, -5.1686, -6.3194, -4.9893, -6.4393, -6.4643, -6.5879, -6.204, -5.371, -6.0614, -4.8993, -5.0749, -4.7757, -5.1551, -4.5498, -5.8042, -5.6781, -4.141, -5.3437, -4.9517, -5.0748, -5.0531, -5.2679, -4.3749, -4.4491, -5.2412, -5.5689, -4.8716, -5.4911, -5.1503, -4.9396, -5.0787, -4.5034, -5.0661, -4.8182, -5.0308, -5.0807, -5.1001, -5.1285, -5.1838, -5.1052, -5.1408]}, \"token.table\": {\"Topic\": [2, 1, 2, 4, 2, 1, 2, 1, 4, 1, 4, 4, 4, 3, 4, 3, 4, 4, 2, 3, 4, 1, 4, 1, 3, 4, 1, 4, 1, 4, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 3, 4, 2, 3, 1, 1, 1, 4, 1, 3, 4, 2, 4, 2, 2, 2, 3, 1, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 1, 2, 3, 3, 3, 4, 2, 3, 4, 3, 1, 4, 2, 2, 2, 1, 3, 4, 1, 2, 2, 3, 1, 2, 3, 4, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 4, 4, 1, 4, 1, 2, 1, 2, 3, 4, 3, 1, 2, 2, 3, 4, 2, 1, 2, 3, 4, 1, 4, 1, 2, 3, 4, 1, 2, 2, 1, 1, 1, 2, 1, 2, 3, 4, 1, 4, 4, 1, 2, 3, 4, 1, 2, 3, 1, 4, 1, 2, 3, 4, 2, 2, 3, 4, 3, 4, 1, 4, 1, 1, 1, 2, 1, 1, 3, 1, 2, 3, 4, 1, 3, 1, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 2, 4, 1, 2, 3, 4, 1, 1, 3, 3, 2, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 2, 3, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 1, 3, 3, 4, 4, 1, 4, 1, 4, 1, 2, 3, 4, 3, 2, 3, 1, 2, 4, 1, 2, 3, 4, 3, 4, 2, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 1, 2, 3, 4, 1, 3, 4, 2, 3, 1, 2, 4, 1, 2, 3, 4, 4, 3, 2, 2, 3, 1, 1, 3, 1, 2, 3, 4, 2, 1, 2, 3, 4, 1, 1, 3, 1, 2, 3, 4, 1, 4, 1, 4, 1, 3, 1, 2, 3, 1, 2, 3, 4, 3, 1, 2, 3, 4, 3, 1, 3, 1, 3, 2, 3, 2, 3, 4, 2, 1, 4, 1, 2, 4, 1, 2, 3, 2, 2, 3, 4, 2, 1, 3, 2, 3, 1, 2, 4, 2, 3, 3, 1, 2, 4, 1, 3, 1, 4, 3, 1, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 2, 3, 3, 1, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 1, 2, 3, 4, 3, 3, 1, 1, 2, 3, 4, 2, 2, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3], \"Freq\": [0.9994614798766882, 0.17925830868294684, 0.07469096195122785, 0.7543787157074013, 0.9957504435973163, 1.008264209831296, 0.9991354885672912, 0.08571861445084504, 0.9086173131789574, 0.03292090352179921, 0.9711666538930768, 0.9971057331148641, 1.0008851093914524, 0.0873602593831047, 0.9172827235225993, 0.9975577563552802, 1.0005751984351587, 1.0007028298029879, 0.8023838235342229, 0.19663160892933526, 0.988074670526255, 0.20293966056627535, 0.7972629522246532, 0.2927100823710912, 0.03598894455382269, 0.669394368701102, 0.08912832286143098, 0.9135653093296675, 0.8081285502926789, 0.1915821994228334, 0.9942177717745406, 0.21800265208881223, 0.6786211589216251, 0.028129374463072544, 0.0773557797734495, 1.0059399994325282, 0.5914791933114445, 0.19933829166849787, 0.17319556489230142, 0.03267840847024556, 0.20540239832460097, 0.09628237421465671, 0.699651919293172, 0.23423798084595435, 0.7636158175578112, 0.9978682498442195, 0.9948342124224974, 0.7049534342328739, 0.2955381705053202, 0.3521838779933381, 0.030816089324417085, 0.6163217864883417, 0.8201418950487513, 0.18283418042488087, 1.0014485551539227, 1.0061725683319473, 0.8620915848867932, 0.14021971561411697, 0.17398476826526346, 0.8241383759933532, 0.9966705857106857, 0.044967782198882596, 0.12990692635232748, 0.7944308188469258, 0.02997852146592173, 0.4637115957763917, 0.13683292990123033, 0.14253430198044828, 0.2565617435648069, 0.5947638354182366, 0.40470772259661963, 0.716896804618628, 0.05227372533677496, 0.23523176401548734, 0.9964665798301059, 0.013405178507417775, 0.9919832095489153, 0.23132523784103515, 0.7109019504383031, 0.05642078971732564, 1.001276176569249, 0.9604151157436042, 0.031749260024581956, 1.0005810483413298, 0.9981200289032942, 0.9995932830969516, 0.12448525661982308, 0.8571699098679246, 0.017783608088546156, 0.04114060130659533, 0.9576617748590803, 1.0025013924381057, 0.9941839882438578, 0.5339403209991757, 0.09807067120393022, 0.019614134240786042, 0.35087506808517255, 0.0768923332514939, 0.9227079990179268, 0.15423331790185146, 0.08652112955469717, 0.13918616493581717, 0.6206950598489144, 0.06555259182976624, 0.012291110968081168, 0.1188140726914513, 0.8071162869039967, 0.02163755931839957, 0.9736901693279807, 0.998240591782826, 0.10153491916087286, 0.8993092839963025, 1.0024909926146504, 1.000393412365741, 0.32753937605001004, 0.11888466241815179, 0.4124569920629756, 0.14314683842185624, 1.0026049979663512, 0.9960644803643214, 0.999985571067939, 0.9991952542193873, 0.9951159290869867, 0.9963749330719552, 0.9992986426360055, 0.4836721696622289, 0.15505280080081024, 0.08794039448404162, 0.2707638461745492, 0.46756796788767224, 0.5321911829615781, 0.5258559513816179, 0.20339711327024843, 0.08764265856360298, 0.18355349246339492, 0.9958329063722169, 1.0005524965145178, 1.0026201853147778, 1.0010569098099993, 1.0073802874554474, 0.9963931881504552, 1.0008028105561724, 0.1697688467330005, 0.20210576992023868, 0.5699382711750731, 0.06063173097607161, 0.996226794481622, 0.9954756652369933, 0.9963508256306235, 0.06667958888857756, 0.06667958888857756, 0.08719638546967834, 0.7796382700818298, 0.0058922498510589165, 0.9663289755736623, 0.027104349314871014, 0.048953843857195596, 0.954599955215314, 0.13271090011716588, 0.4163479219362067, 0.3538957336457757, 0.0962804569477478, 1.002475496749273, 0.5954607162634855, 0.32141345480131317, 0.08458248810560873, 1.0067709099605968, 1.0020707561583388, 0.03141651025119883, 0.9739118177871636, 1.0004283747660028, 0.9945342687538783, 1.0064289536543516, 0.9984263461167487, 0.9944241557132603, 0.9963377630381106, 1.00664604862388, 0.5354128939949435, 0.2313662053236068, 0.03997426584125144, 0.19381462226061302, 1.0006402722039904, 0.9991704862200598, 0.9999648945562445, 0.6191029138967361, 0.06766152064445204, 0.314626070996702, 0.5101431223782376, 0.20980534047104982, 0.07616221263675096, 0.204057248951295, 0.17368459837524436, 0.7128305391650653, 0.06151329525789904, 0.05427643699226386, 0.7949894936124864, 0.046374387127395035, 0.15899789872249728, 0.016929203186269264, 0.9818937848036174, 0.41822252494751905, 0.3947268774785573, 0.11982780209170489, 0.06578781291309288, 0.992000437100964, 0.9889912046536181, 0.007380531378012076, 0.997860166942246, 1.0022542432989394, 0.02883463059464958, 0.9721389743339002, 0.040057129477948164, 0.9613711074707558, 0.7881600541798545, 0.13707131377040946, 0.07424696162563846, 0.342082673660445, 0.20441525621172932, 0.05840435891763695, 0.39422942269404937, 0.018182683124335023, 0.9818648887140913, 1.0031601291053924, 0.25125645481107334, 0.2214463669521324, 0.19376557108311587, 0.3321695504281986, 0.8287065343061542, 0.03282006076460016, 0.049230091146900246, 0.09025516710265044, 0.4235034271166246, 0.07377156472354106, 0.0546456034989193, 0.4480939486911383, 0.9876981350024345, 0.011484862034912028, 0.01645634679961641, 0.9832667212770806, 0.9989383513759518, 0.9976261968438591, 0.9979122315946422, 0.9988305557656438, 1.000973619146991, 0.03407406387447793, 0.9654318097768746, 0.1912539399935721, 0.09904221892524269, 0.07513547642604618, 0.6386515496213925, 0.9992909649103443, 0.023080501592049826, 0.977074567396776, 0.7678281849703354, 0.05295366792898865, 0.17474710416566255, 0.34289057281593677, 0.21364720306223753, 0.4167439269609078, 0.029013817699810034, 0.10243509102382511, 0.8999654425664635, 0.012795289336490772, 0.9852372789097895, 1.0013239545408206, 0.9979924934331978, 1.0050833906044345, 0.9753496383500293, 0.02167443640777843, 0.9570588046771231, 0.03728800537703077, 0.996736477618021, 0.9998855597856129, 0.5311860668344247, 0.04259510913294915, 0.010022378619517448, 0.41592871270997406, 0.17108883791372967, 0.08259461140662812, 0.7433515026596531, 1.0032621451071069, 1.0028520909958887, 0.16336824566443175, 0.5888063854155561, 0.2518593787326656, 0.2588731189028847, 0.28718736628288777, 0.044493817311433315, 0.408534140768615, 1.0071484713798324, 1.001672690267794, 1.0008789046880215, 0.6681244454796033, 0.3331264742167209, 1.0045185238428136, 0.9958566410187296, 1.00362436454773, 0.0860152919387514, 0.032255734477031776, 0.02150382298468785, 0.860152919387514, 0.9998658724777365, 0.3216371470648631, 0.18294956989010563, 0.05901599028713085, 0.4367183281247683, 0.9969414004932723, 0.9886501571593421, 0.010746197360427631, 0.7198724075498986, 0.007824700082064115, 0.0547729005744488, 0.2190916022977952, 0.26182402594274495, 0.7390728833573686, 0.03829771570266392, 0.957442892566598, 0.06231328759736481, 0.9346993139604722, 0.09322314738641616, 0.1864462947728323, 0.7147107966291905, 0.5214051796494533, 0.08860480177049533, 0.04089452389407477, 0.34419557610846263, 0.9991739609838751, 0.16218282569873332, 0.6837978597027675, 0.13588290801785766, 0.021916598067396396, 1.0044112977389899, 1.0067498006513396, 0.998780105231168, 0.98490110749604, 0.014700016529791641, 0.0496278068813805, 0.9429283307462295, 0.004166753200180558, 0.5875122012254588, 0.4083418136176947, 1.0037285837548258, 0.07554309173835387, 0.9065171008602465, 0.5073831596286486, 0.1076267308303194, 0.38438118153685497, 0.09846589109529161, 0.7233455845846422, 0.18178318356053835, 1.0015392784677886, 0.09442042860492085, 0.9098695847383282, 0.9921943460951398, 1.004262012983271, 0.9943214005581441, 0.9991336735677125, 0.9993177870780215, 1.0007115018868546, 0.41879650212083974, 0.07312319878300376, 0.5085386097181626, 0.018127874072178092, 0.9817674958037503, 0.9995241601194024, 0.8724721909603956, 0.009803058325397703, 0.11763669990477243, 0.058126601540936795, 0.9397133915784782, 0.014541600179811983, 0.9815580121373089, 0.996232707170006, 0.9077923342607779, 0.0546862851964324, 0.043749028157145924, 0.728947482886989, 0.14798181983420075, 0.021923232568029742, 0.09865454655613384, 0.35124136207286155, 0.4076908666917143, 0.2414784364250923, 0.44174370229295856, 0.09245798420085179, 0.1294411778811925, 0.3369579868653265, 0.08043964911252474, 0.9193102755717113, 1.0046118815674607, 1.0095834932003012, 0.042884546533952544, 0.9488205920637, 0.005360568316744068, 0.5625087871673897, 0.0913822939441608, 0.04873722343688575, 0.29851549355092527, 0.525756867114205, 0.03953059151234624, 0.027671414058642367, 0.40716509257716627, 0.42042978983624596, 0.15232963399863986, 0.26200697047766053, 0.16451600471853103, 0.33272536834104444, 0.0601737368276357, 0.010618894734288653, 0.5981977366982608, 0.9967887952605716, 1.004331132624211, 0.5492171227621153, 0.010929693985315729, 0.15848056278707806, 0.2787071966255511, 0.12772077544628477, 0.4497595878215599, 0.22807281329693707, 0.19431803692899038, 0.8398846181415776, 0.1574783659015458, 0.014550789844597692, 0.24736342735816078, 0.08245447578605358, 0.6596358062884287, 0.9979180724425959, 1.0068399609915626, 1.0031529737395541, 0.1205517701125182, 0.7916232904055363, 0.020091961685419702, 0.06831266973042699, 1.0008106173210038, 1.0022626122606093, 0.47645495960575285, 0.27639652352819916, 0.05791165254876554, 0.18689669686192514, 1.0002030148358616, 0.41700840917971244, 0.10710052039041794, 0.09570684800845859, 0.3782699230810506, 0.395401412514519, 0.16045274710734103, 0.11460910507667217, 0.32854610121979355, 0.9883123189297685, 0.009412498275521604, 0.21477882925568367, 0.4033069127134505, 0.2648938894153432, 0.1169351403725389, 0.5564975050234396, 0.0763820104934133, 0.16731297536652434, 0.20004812272084432, 0.525818548917598, 0.14618192023850235, 0.11563644436777053, 0.212363784625088, 0.3704864889046794, 0.009036255826943401, 0.6212425881023588], \"Term\": [\"3d\", \"accept\", \"accept\", \"accept\", \"algorithm\", \"allah\", \"amiga\", \"animal\", \"animal\", \"anti\", \"anti\", \"argue\", \"argument\", \"assumption\", \"assumption\", \"astronaut\", \"atheism\", \"atheist\", \"available\", \"available\", \"behavior\", \"belief\", \"belief\", \"believe\", \"believe\", \"believe\", \"believer\", \"believer\", \"bible\", \"bible\", \"biblical\", \"bit\", \"bit\", \"bit\", \"bit\", \"bobby\", \"book\", \"book\", \"book\", \"book\", \"cause\", \"cause\", \"cause\", \"center\", \"center\", \"child\", \"christ\", \"christian\", \"christian\", \"claim\", \"claim\", \"claim\", \"code\", \"code\", \"color\", \"comp\", \"computer\", \"computer\", \"conclusion\", \"conclusion\", \"consequence\", \"cost\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"could\", \"data\", \"data\", \"day\", \"day\", \"day\", \"dc\", \"definition\", \"definition\", \"design\", \"design\", \"design\", \"development\", \"die\", \"die\", \"directory\", \"display\", \"driver\", \"earth\", \"earth\", \"earth\", \"edu\", \"edu\", \"email\", \"energy\", \"even\", \"even\", \"even\", \"even\", \"evidence\", \"evidence\", \"example\", \"example\", \"example\", \"example\", \"exist\", \"exist\", \"exist\", \"exist\", \"existence\", \"existence\", \"fallacy\", \"false\", \"false\", \"father\", \"file\", \"first\", \"first\", \"first\", \"first\", \"flight\", \"follower\", \"format\", \"ftp\", \"fund\", \"gay\", \"gif\", \"give\", \"give\", \"give\", \"give\", \"god\", \"god\", \"good\", \"good\", \"good\", \"good\", \"gospel\", \"graphic\", \"graphics\", \"greek\", \"hang\", \"heaven\", \"hi\", \"high\", \"high\", \"high\", \"high\", \"holy\", \"homosexual\", \"homosexuality\", \"human\", \"human\", \"human\", \"human\", \"image\", \"image\", \"image\", \"imply\", \"imply\", \"include\", \"include\", \"include\", \"include\", \"info\", \"information\", \"information\", \"information\", \"institute\", \"islam\", \"islamic\", \"islamic\", \"jesus\", \"jew\", \"jewish\", \"jpeg\", \"juda\", \"king\", \"km\", \"know\", \"know\", \"know\", \"know\", \"koresh\", \"launch\", \"lds\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"live\", \"live\", \"live\", \"logic\", \"logic\", \"look\", \"look\", \"look\", \"look\", \"lord\", \"love\", \"love\", \"lunar\", \"mac\", \"mail\", \"mail\", \"majority\", \"majority\", \"man\", \"man\", \"man\", \"many\", \"many\", \"many\", \"many\", \"mar\", \"mar\", \"matthew\", \"may\", \"may\", \"may\", \"may\", \"maybe\", \"maybe\", \"maybe\", \"maybe\", \"mean\", \"mean\", \"mean\", \"mean\", \"men\", \"men\", \"mission\", \"mission\", \"moon\", \"moral\", \"morality\", \"mormon\", \"murder\", \"muslim\", \"muslim\", \"must\", \"must\", \"must\", \"must\", \"nasa\", \"national\", \"national\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"objective\", \"objective\", \"office\", \"office\", \"orbit\", \"orbital\", \"orbiter\", \"package\", \"package\", \"passage\", \"passage\", \"payload\", \"pc\", \"people\", \"people\", \"people\", \"people\", \"person\", \"person\", \"person\", \"pixel\", \"planetary\", \"please\", \"please\", \"please\", \"point\", \"point\", \"point\", \"point\", \"premise\", \"probe\", \"processing\", \"program\", \"program\", \"prophecy\", \"prophet\", \"propulsion\", \"prove\", \"prove\", \"prove\", \"prove\", \"pub\", \"question\", \"question\", \"question\", \"question\", \"quran\", \"ra\", \"ra\", \"really\", \"really\", \"really\", \"really\", \"religion\", \"religion\", \"religious\", \"religious\", \"report\", \"report\", \"research\", \"research\", \"research\", \"right\", \"right\", \"right\", \"right\", \"rocket\", \"run\", \"run\", \"run\", \"run\", \"russian\", \"satan\", \"satellite\", \"saw\", \"saw\", \"sci\", \"sci\", \"science\", \"science\", \"science\", \"screen\", \"secular\", \"secular\", \"seem\", \"seem\", \"seem\", \"send\", \"send\", \"send\", \"server\", \"service\", \"service\", \"sex\", \"sgi\", \"shall\", \"shuttle\", \"software\", \"solar\", \"something\", \"something\", \"something\", \"space\", \"space\", \"spacecraft\", \"speak\", \"speak\", \"speak\", \"star\", \"star\", \"statement\", \"statement\", \"station\", \"story\", \"story\", \"story\", \"sure\", \"sure\", \"sure\", \"sure\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"technology\", \"technology\", \"telescope\", \"testament\", \"thanks\", \"thanks\", \"thanks\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"time\", \"true\", \"true\", \"true\", \"true\", \"tyre\", \"unix\", \"us\", \"us\", \"us\", \"us\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"value\", \"value\", \"value\", \"value\", \"vehicle\", \"venus\", \"verse\", \"version\", \"version\", \"version\", \"version\", \"video\", \"viewer\", \"want\", \"want\", \"want\", \"want\", \"washington\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"window\", \"window\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"would\", \"would\", \"would\", \"would\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el241011122505458803128538301\", ldavis_el241011122505458803128538301_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el241011122505458803128538301\", ldavis_el241011122505458803128538301_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el241011122505458803128538301\", ldavis_el241011122505458803128538301_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                x           y  topics  cluster       Freq\n",
       "topic                                                    \n",
       "1      -78.671196 -396.283112       1        1  28.387236\n",
       "0      -33.778664   -7.443441       2        1  25.969431\n",
       "3      138.194778 -224.309540       3        1  23.266617\n",
       "2     -250.644653 -179.417053       4        1  22.376716, topic_info=     Category         Freq        Term        Total  loglift  logprob\n",
       "term                                                                 \n",
       "846   Default  1048.000000       space  1048.000000  30.0000  30.0000\n",
       "415   Default   848.000000       image   848.000000  29.0000  29.0000\n",
       "328   Default   545.000000        file   545.000000  28.0000  28.0000\n",
       "575   Default   415.000000        nasa   415.000000  27.0000  27.0000\n",
       "477   Default   399.000000      launch   399.000000  26.0000  26.0000\n",
       "367   Default   789.000000         god   789.000000  25.0000  25.0000\n",
       "268   Default   437.000000         edu   437.000000  24.0000  24.0000\n",
       "449   Default   406.000000       jesus   406.000000  23.0000  23.0000\n",
       "778   Default   285.000000   satellite   285.000000  22.0000  22.0000\n",
       "833   Default   297.000000    software   297.000000  21.0000  21.0000\n",
       "373   Default   294.000000     graphic   294.000000  20.0000  20.0000\n",
       "700   Default   534.000000     program   534.000000  19.0000  19.0000\n",
       "648   Default   798.000000      people   798.000000  18.0000  18.0000\n",
       "74    Default   253.000000     atheist   253.000000  17.0000  17.0000\n",
       "344   Default   280.000000      format   280.000000  16.0000  16.0000\n",
       "623   Default   253.000000       orbit   253.000000  15.0000  15.0000\n",
       "63    Default   245.000000    argument   245.000000  14.0000  14.0000\n",
       "455   Default   260.000000        jpeg   260.000000  13.0000  13.0000\n",
       "350   Default   256.000000         ftp   256.000000  12.0000  12.0000\n",
       "554   Default   243.000000     mission   243.000000  11.0000  11.0000\n",
       "156   Default   246.000000       color   246.000000  10.0000  10.0000\n",
       "905   Default   758.000000       think   758.000000   9.0000   9.0000\n",
       "207   Default   447.000000        data   447.000000   8.0000   8.0000\n",
       "996   Default   442.000000        year   442.000000   7.0000   7.0000\n",
       "95    Default   416.000000     believe   416.000000   6.0000   6.0000\n",
       "73    Default   198.000000     atheism   198.000000   5.0000   5.0000\n",
       "513   Default   242.000000        mail   242.000000   4.0000   4.0000\n",
       "814   Default   201.000000     shuttle   201.000000   3.0000   3.0000\n",
       "263   Default   281.000000       earth   281.000000   2.0000   2.0000\n",
       "747   Default   301.000000    religion   301.000000   1.0000   1.0000\n",
       "...       ...          ...         ...          ...      ...      ...\n",
       "747    Topic4   222.579750    religion   301.729376   1.1929  -4.7757\n",
       "410    Topic4   152.302203       human   194.962210   1.2502  -5.1551\n",
       "95     Topic4   278.976697     believe   416.794663   1.0957  -4.5498\n",
       "707    Topic4    79.577483       prove    93.006718   1.3412  -5.8042\n",
       "176    Topic4    90.274410  conclusion   109.204962   1.3068  -5.6781\n",
       "367    Topic4   419.859180         god   789.190076   0.8661  -4.1410\n",
       "655    Topic4   126.126410      person   169.502583   1.2016  -5.3437\n",
       "573    Topic4   186.653611        must   292.804425   1.0469  -4.9517\n",
       "298    Topic4   165.031260     example   265.831019   1.0204  -5.0748\n",
       "923    Topic4   168.652017        true   282.515278   0.9813  -5.0531\n",
       "950    Topic4   136.049517       value   206.174375   1.0814  -5.2679\n",
       "648    Topic4   332.317917      people   798.213708   0.6209  -4.3749\n",
       "905    Topic4   308.536925       think   758.905922   0.5971  -4.4491\n",
       "149    Topic4   139.731282       claim   227.154066   1.0112  -5.2412\n",
       "3      Topic4   100.694809      accept   133.885007   1.2123  -5.5689\n",
       "673    Topic4   202.209007       point   494.450720   0.6030  -4.8716\n",
       "127    Topic4   108.840312       cause   155.791754   1.1385  -5.4911\n",
       "837    Topic4   153.028279   something   300.862112   0.8211  -5.1503\n",
       "519    Topic4   188.933269        many   479.416272   0.5660  -4.9396\n",
       "532    Topic4   164.390833        mean   365.994677   0.6968  -5.0787\n",
       "992    Topic4   292.224011       would  1374.999040  -0.0515  -4.5034\n",
       "972    Topic4   166.481233         way   438.840071   0.5279  -5.0661\n",
       "943    Topic4   213.301457         use  1096.141168  -0.1397  -4.8182\n",
       "974    Topic4   172.461778        well   523.518615   0.3868  -5.0308\n",
       "886    Topic4   164.058048        take   486.707561   0.4097  -5.0807\n",
       "289    Topic4   160.908979        even   458.852779   0.4493  -5.1001\n",
       "530    Topic4   156.411718         may   469.639676   0.3977  -5.1285\n",
       "716    Topic4   147.987994    question   338.891204   0.6686  -5.1838\n",
       "467    Topic4   160.090156        know   825.531109  -0.1431  -5.1052\n",
       "885    Topic4   154.487129      system   637.738103   0.0793  -5.1408\n",
       "\n",
       "[280 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "0         2  0.999461          3d\n",
       "3         1  0.179258      accept\n",
       "3         2  0.074691      accept\n",
       "3         4  0.754379      accept\n",
       "24        2  0.995750   algorithm\n",
       "25        1  1.008264       allah\n",
       "37        2  0.999135       amiga\n",
       "41        1  0.085719      animal\n",
       "41        4  0.908617      animal\n",
       "46        1  0.032921        anti\n",
       "46        4  0.971167        anti\n",
       "62        4  0.997106       argue\n",
       "63        4  1.000885    argument\n",
       "68        3  0.087360  assumption\n",
       "68        4  0.917283  assumption\n",
       "70        3  0.997558   astronaut\n",
       "73        4  1.000575     atheism\n",
       "74        4  1.000703     atheist\n",
       "80        2  0.802384   available\n",
       "80        3  0.196632   available\n",
       "92        4  0.988075    behavior\n",
       "94        1  0.202940      belief\n",
       "94        4  0.797263      belief\n",
       "95        1  0.292710     believe\n",
       "95        3  0.035989     believe\n",
       "95        4  0.669394     believe\n",
       "96        1  0.089128    believer\n",
       "96        4  0.913565    believer\n",
       "100       1  0.808129       bible\n",
       "100       4  0.191582       bible\n",
       "...     ...       ...         ...\n",
       "968       1  0.476455        want\n",
       "968       2  0.276397        want\n",
       "968       3  0.057912        want\n",
       "968       4  0.186897        want\n",
       "970       3  1.000203  washington\n",
       "972       1  0.417008         way\n",
       "972       2  0.107101         way\n",
       "972       3  0.095707         way\n",
       "972       4  0.378270         way\n",
       "974       1  0.395401        well\n",
       "974       2  0.160453        well\n",
       "974       3  0.114609        well\n",
       "974       4  0.328546        well\n",
       "981       2  0.988312      window\n",
       "981       3  0.009412      window\n",
       "988       1  0.214779        work\n",
       "988       2  0.403307        work\n",
       "988       3  0.264894        work\n",
       "988       4  0.116935        work\n",
       "990       1  0.556498       world\n",
       "990       2  0.076382       world\n",
       "990       3  0.167313       world\n",
       "990       4  0.200048       world\n",
       "992       1  0.525819       would\n",
       "992       2  0.146182       would\n",
       "992       3  0.115636       would\n",
       "992       4  0.212364       would\n",
       "996       1  0.370486        year\n",
       "996       2  0.009036        year\n",
       "996       3  0.621243        year\n",
       "\n",
       "[465 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 4, 3])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "visualization_panel = pyLDAvis.sklearn.prepare(lda_news, bow_news_corpus, bow_vectorizer_news, mds='tsne')\n",
    "visualization_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 3:**\n",
    "    \n",
    "Use the visualization and no_topics_news = 4 to answer the following question: Which topic is the most common topic in the corpus (toy can give a name to that topic)? List 5 most relevant and exclusive terms for that topic (with lambda = 0.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space is the most common one.\n",
    "space, nasa, launch, satellite, orbit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
