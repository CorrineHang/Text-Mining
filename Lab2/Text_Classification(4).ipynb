{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION\n",
    "\n",
    "We will classify consumer posts based on the topic of the message.\n",
    "\n",
    "We'll use a dataset containing 18,000 posts (messages) on 20 topics. It's part of the sklearn dataset collection. Posts are devided into \"train\" and \"test\" types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the modules we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (1.14.3)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /anaconda3/lib/python3.6/site-packages (0.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda3/lib/python3.6/site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.6/site-packages (from pandas) (2018.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from pandas) (1.14.3)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "!{sys.executable} -m pip install sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 20 available, we will use posts on 4 topics (classes) only: atheism, religion, computer graphics, and science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(categories = categories,\n",
    "                                  subset = 'train', \n",
    "                                  shuffle = False, \n",
    "                                  remove = ('headers', 'footers', 'quotes')) \n",
    "\n",
    "twenty_test = fetch_20newsgroups(categories = categories,\n",
    "                                 subset='test', \n",
    "                                 shuffle=False,\n",
    "                                 remove=('headers', 'footers', 'quotes')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the training data. First, have a look at one of the posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->\tFirst I want to start right out and say that I'm a Christian.  It \n",
      "->makes sense to be one.  Have any of you read Tony Campollo's book- liar, \n",
      "->lunatic, or the real thing?  (I might be a little off on the title, but he \n",
      "->writes the book.  Anyway he was part of an effort to destroy Christianity, \n",
      "->in the process he became a Christian himself.\n",
      "\n",
      "Sounds like you are saying he was a part of some conspiracy.  Just what organization did he \n",
      "belong to? Does it have a name?\n",
      "\n",
      "->\tThe book says that Jesus was either a liar, or he was crazy ( a \n",
      "->modern day Koresh) or he was actually who he said he was.\n",
      "\n",
      "Logic alert - artificial trifercation.  The are many other possible explainations.  Could have been\n",
      "that he never existed.  There have been some good points made in this group that is not \n",
      "impossible  that JC is an amalgam of a number of different myths, Mithra comes to mind.\n",
      "\n",
      "->\tSome reasons why he wouldn't be a liar are as follows.  Who would \n",
      "->die for a lie?  Wouldn't people be able to tell if he was a liar?  People \n",
      "->gathered around him and kept doing it, many gathered from hearing or seeing \n",
      "->someone who was or had been healed.  Call me a fool, but I believe he did \n",
      "->heal people.  \n",
      "\n",
      "\n",
      "Logic alert -  argument from incredulity.  Just because it is hard for you to believe this doesn't\n",
      "mean that it isn't true.  Liars can be very pursuasive, just look at Koresh that you yourself site.\n",
      "He has followers that don't think he is a fake and they have shown that they are willing to die.\n",
      "By not giving up after getting shot himself, Koresh has shown that he too is will to die for what \n",
      "he believes.  As far as healing goes.  If I rememer right the healing that was attributed is not\n",
      "consistent between the different gospels.  In one of them the healing that is done is not any more \n",
      "that faith healers can pull off today.  Seems to me that the early gospels weren't that compeling,\n",
      "so the stories got bigger to appeal better.\n",
      "\n",
      "->\tNiether was he a lunatic.  Would more than an entire nation be drawn \n",
      "->to someone who was crazy.  Very doubtful, in fact rediculous.  For example \n",
      "->anyone who is drawn to David Koresh is obviously a fool, logical people see \n",
      "->this right away.\n",
      "->\tTherefore since he wasn't a liar or a lunatic, he must have been the \n",
      "->real thing.  \n",
      "\n",
      "\n",
      "Or might not have existed, or any number of things.  That is the logical pitfall that those who\n",
      "use flawed logic like this fall into.  There are bifurcations (or tri, quad, etc) that are valid, because\n",
      "in the proceeding steps, the person shows conclusively that the alternatives are all that are \n",
      "possible.  Once everyone agrees that the given set is indeed all there are, then arguments among\n",
      "the alternatives can be presentent, and one mostly likely to be true can be deduced by excluding\n",
      "all other possible alternatives.\n",
      "\n",
      "However, if it can be shown that the set is not all inclusive, then any conclusions bases on the \n",
      "incomplete set are invalid, even if the true choice is one of the original choices.  I have given at \n",
      "least one valid alternative, so the conclusion that JC is the real McCoy just because he isn't one of\n",
      "the other two alternative is no longer valid.\n",
      "\n",
      "->\tSome other things to note.  He fulfilled loads of prophecies in \n",
      "->the psalms, Isaiah and elsewhere in 24 hrs alone.  This in his betrayal \n",
      "->and Crucifixion.  I don't have my Bible with me at this moment, next time I \n",
      "->write I will use it.\n",
      "\n",
      "JC was a rabbi.  He knew what those prophecies were.  It wouldn't be any great shakes to make\n",
      "sure one does a list of actions that would fullfill prophecy.  What would be compeling is if there\n",
      "were a set of clear and explicit prophecies AND JC had absolutely NO knowledge of then,  yet \n",
      "fullfilled them anyway.\n",
      "\n",
      "->\tI don't think most people understand what a Christian is.  It \n",
      "->is certainly not what I see a lot in churches.  Rather I think it \n",
      "->should be a way of life, and a total sacrafice of everything for God's \n",
      "->sake.  He loved us enough to die and save us so we should do the \n",
      "->same.  Hey we can't do it, God himself inspires us to turn our lives \n",
      "->over to him.  That's tuff and most people don't want to do it, to be a \n",
      "->real Christian would be something for the strong to persevere at.  But \n",
      "->just like weight lifting or guitar playing, drums, whatever it takes \n",
      "->time.  We don't rush it in one day, Christianity is your whole life.  \n",
      "->It is not going to church once a week, or helping poor people once in \n",
      "->a while.  We box everything into time units.  Such as work at this \n",
      "->time, sports, Tv, social life.  God is above these boxes and should be \n",
      "->carried with us into all these boxes that we have created for \n",
      "->ourselves.  \n",
      "\n",
      "Here I agree with you.  Anyone who buys into this load of mythology should take what it says \n",
      "seriously, and what it says is that it must be a total way of life.  I have very little respect for \n",
      "Xians that don't.  If the myth is true, then it is true in its entirity.  The picking and choosing\n",
      "that I see a lot of leaves a bad taste in my mouth.\n",
      "\n",
      "Jim\t  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "James L. Felder\t\t\t|\n",
      "Sverdrup Technology,Inc.\t|     phone: 216-891-4019\n",
      "NASA Lewis Research Center     \t|    \n",
      "Cleveland, Ohio  44135         \t|     email: jfelder@lerc.nasa.gov \n",
      "\"Some people drink from the fountain of knowledge, other people gargle\"\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[7])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes (topics) for each message, that you will be predicting, are encoded as numbers and can accessed via attribute .target and their names can be accessed via .target_names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category names:  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "Categories for first 10 observations:  [0 2 1 0 2 3 0 0 2 2]\n",
      "Number of posts in the training dataset:  2034\n"
     ]
    }
   ],
   "source": [
    "print(\"Category names: \", twenty_train.target_names)    \n",
    "print(\"Categories for first 10 observations: \", twenty_train.target[:10])     \n",
    "print(\"Number of posts in the training dataset: \", twenty_train.filenames.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to be used later for feature matrix description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmat_descr_fun(your_feature_matrix,your_vectorizer):\n",
    "    print(\"Dimensions (number of posts x number of features): \", your_feature_matrix.shape)  \n",
    "    print(\"The first 5 features - names: \", your_vectorizer.get_feature_names()[0:5]) \n",
    "    print(\"Share of non-zero elements in the matrix: \", \n",
    "          your_feature_matrix.nnz / (float(your_feature_matrix.shape[0]) * float(your_feature_matrix.shape[1])))\n",
    "    print(\"Average number of features present, per post: \", \n",
    "          round(your_feature_matrix.nnz/float(your_feature_matrix.shape[0]),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FEATURE EXTRACTION\n",
    "\n",
    "Let's do feature extraction for our TRAIN data using the \"bag-of-words\" and TF-IDF methods. We'll use the option stop_words = 'english' to remove stopwords from the set of features. \n",
    "\n",
    "**First, let's do the TF_IDF method for TRAIN data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (number of posts x number of features):  (2034, 26576)\n",
      "The first 5 features - names:  ['00', '000', '0000', '00000', '000000']\n",
      "Share of non-zero elements in the matrix:  0.002472159028010871\n",
      "Average number of features present, per post:  65.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', norm = 'l2')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(twenty_train.data)\n",
    "\n",
    "fmat_descr_fun(X_train_tfidf,tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first 5 rows of the TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>000005102000</th>\n",
       "      <th>000062david42</th>\n",
       "      <th>0001</th>\n",
       "      <th>000100255pixel</th>\n",
       "      <th>00041032</th>\n",
       "      <th>...</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurvanism</th>\n",
       "      <th>zus</th>\n",
       "      <th>zvi</th>\n",
       "      <th>zwaartepunten</th>\n",
       "      <th>zwak</th>\n",
       "      <th>zwakke</th>\n",
       "      <th>zware</th>\n",
       "      <th>zwarte</th>\n",
       "      <th>zyxel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26576 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  0000  00000  000000  000005102000  000062david42  0001  \\\n",
       "0  0.0  0.0   0.0    0.0     0.0           0.0            0.0   0.0   \n",
       "1  0.0  0.0   0.0    0.0     0.0           0.0            0.0   0.0   \n",
       "2  0.0  0.0   0.0    0.0     0.0           0.0            0.0   0.0   \n",
       "3  0.0  0.0   0.0    0.0     0.0           0.0            0.0   0.0   \n",
       "4  0.0  0.0   0.0    0.0     0.0           0.0            0.0   0.0   \n",
       "\n",
       "   000100255pixel  00041032  ...    zurich  zurvanism  zus  zvi  \\\n",
       "0             0.0       0.0  ...       0.0        0.0  0.0  0.0   \n",
       "1             0.0       0.0  ...       0.0        0.0  0.0  0.0   \n",
       "2             0.0       0.0  ...       0.0        0.0  0.0  0.0   \n",
       "3             0.0       0.0  ...       0.0        0.0  0.0  0.0   \n",
       "4             0.0       0.0  ...       0.0        0.0  0.0  0.0   \n",
       "\n",
       "   zwaartepunten  zwak  zwakke  zware  zwarte  zyxel  \n",
       "0            0.0   0.0     0.0    0.0     0.0    0.0  \n",
       "1            0.0   0.0     0.0    0.0     0.0    0.0  \n",
       "2            0.0   0.0     0.0    0.0     0.0    0.0  \n",
       "3            0.0   0.0     0.0    0.0     0.0    0.0  \n",
       "4            0.0   0.0     0.0    0.0     0.0    0.0  \n",
       "\n",
       "[5 rows x 26576 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features_names = tfidf_vectorizer.get_feature_names() \n",
    "X_train_tfidf_table = pd.DataFrame(data = X_train_tfidf.todense(), columns = tfidf_features_names)\n",
    "X_train_tfidf_table.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the TEST dataset using the TF_IDF method. \n",
    "\n",
    "**IMPORTANT: For transforming test data, you'll use the feature names extracted for the train data and do the counts for those feature names using the test data (you do not create new feature names based on the test data). There, 1) we do not define a new vectorizer and 2) we use method .transform (not .fit_trandform) with our vectorizer on the test data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (number of posts x number of features):  (1353, 26576)\n",
      "The first 5 features - names:  ['00', '000', '0000', '00000', '000000']\n",
      "Share of non-zero elements in the matrix:  0.0023848546254604903\n",
      "Average number of features present, per post:  63.4\n"
     ]
    }
   ],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(twenty_test.data)\n",
    "fmat_descr_fun(X_test_tfidf,tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's do the \"bag-of-words\" method now for the TRAIN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(stop_words = 'english') \n",
    "X_train_bow = bow_vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe the resulting matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (number of posts x number of features):  (2034, 26576)\n",
      "The first 5 features - names:  ['00', '000', '0000', '00000', '000000']\n",
      "Share of non-zero elements in the matrix:  0.002472159028010871\n",
      "Average number of features present, per post:  65.7\n"
     ]
    }
   ],
   "source": [
    "fmat_descr_fun(X_train_bow,bow_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE: Transform the TEST dataset using the \"bag-of-words\" method. IMPORTANT: For transforming test data, you'll use the feature names extracted for the train data and do the counts for those feature names using the test data (you do not create new feature names based on the test data). There, 1) we do not define a new vectorizer and 2) we use method .transform (not .fit_trandform) with our vectorizer on the test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (number of posts x number of features):  (1353, 26576)\n",
      "The first 5 features - names:  ['00', '000', '0000', '00000', '000000']\n",
      "Share of non-zero elements in the matrix:  0.0023848546254604903\n",
      "Average number of features present, per post:  63.4\n"
     ]
    }
   ],
   "source": [
    "X_test_bow = bow_vectorizer.transform(twenty_test.data)\n",
    "fmat_descr_fun(X_test_bow,bow_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NAIVE BAYES CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's classify the posts using Naive Bayes classifier with TF-IDF featire matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0.1) \n",
    "clf.fit(X_train_tfidf, twenty_train.target)\n",
    "predicted_nb = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can set the hyperparameter alpha to an optimal value by trying different values > 0. With alpha = 0, you model will assign a probability of zero to a document in the test data if the document contains a feature not found in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the predictive power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 224             11         35                  49\n",
      "comp.graphics                 7            358         23                   1\n",
      "sci.space                    21             17        353                   3\n",
      "talk.religion.misc           84              9         23                 135 \n",
      "\n",
      "Accuracy rate:  0.7908351810790836 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(twenty_test.target, predicted_nb)\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm, \n",
    "                                           columns = twenty_train.target_names,\n",
    "                                           index = twenty_train.target_names),\"\\n\")\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(twenty_test.target, predicted_nb),\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE: Do the Naive Bayes Classifier with \"bag-of-words\" features in the cell below and compare its performance to the classifier using TF_IDF features:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 227              4         28                  60\n",
      "comp.graphics                11            351         24                   3\n",
      "sci.space                    19             21        343                  11\n",
      "talk.religion.misc           82              7         21                 141 \n",
      "\n",
      "Accuracy rate:  0.7849223946784922 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_bow, twenty_train.target)\n",
    "predicted_nb_bow = clf.predict(X_test_bow)\n",
    "cm_bow = metrics.confusion_matrix(twenty_test.target, predicted_nb_bow)\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_bow, \n",
    "                                           columns = twenty_train.target_names,\n",
    "                                           index = twenty_train.target_names),\"\\n\")\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(twenty_test.target, predicted_nb_bow),\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SUPPORT VECTOR MACHINES (SVM) CLASSIFIER (for TF-IDF features only) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SVM, set parameter loss = 'hinge' in linear_model.SGDClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_svm = linear_model.SGDClassifier(loss='hinge') \n",
    "clf_svm.fit(X_train_tfidf, twenty_train.target) \n",
    "predicted_svm = clf_svm.predict(X_test_tfidf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the SVM classifier performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 209              9         32                  69\n",
      "comp.graphics                19            348         17                   5\n",
      "sci.space                    38             19        327                  10\n",
      "talk.religion.misc           71             13         17                 150 \n",
      "\n",
      "Accuracy score:  0.7642276422764228 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(twenty_test.target, predicted_svm)\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm, \n",
    "                                           columns = twenty_train.target_names,\n",
    "                                           index = twenty_train.target_names),\"\\n\")\n",
    "print(\"Accuracy score: \", metrics.accuracy_score(twenty_test.target, predicted_svm),\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LOGIT-BASED CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.770879526977088 \n",
      "\n",
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 197             13         51                  58\n",
      "comp.graphics                 8            354         26                   1\n",
      "sci.space                    18             21        354                   1\n",
      "talk.religion.misc           66             16         31                 138 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_log = linear_model.SGDClassifier(loss='log')\n",
    "clf_log.fit(X_train_tfidf, twenty_train.target)\n",
    "predicted_log = clf_log.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy score: \", metrics.accuracy_score(twenty_test.target, predicted_log),\"\\n\") \n",
    "cm = metrics.confusion_matrix(twenty_test.target, predicted_log)                                                     \n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm, \n",
    "                                           columns = twenty_train.target_names,\n",
    "                                           index = twenty_train.target_names), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE: Provide your comments on the performance of:**\n",
    "\n",
    "**1) Naive Bayes classifier with \"bag-of-words\" versus TF-IDF features**\n",
    "\n",
    "**2) Naive Bayes, Logit-Based and SVM classifiers with TF-IDF features. Which of the 3 performed best? Did any classifier perform better at predicting a particular topic compared to others? If a classifier did a mistake and misclassified a \"Computer Graphics\" post, to which class such a post was mistakenly assigned, typically? What about a post on the \"Atheism\" topic?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier with \"bag-of-words\" performs slightly worse than TF-IDF in accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes performs best, especially at alt.atheism. SVM best at talk.religion.misc; Logit-Based at sci.space and comp.graphics. Typically mistakenly assign \"Computer Graphics\" to Sci.space; \"Atheism\" to \"talk.religion.misc\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
