{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION and VECTORIZATION\n",
    "\n",
    "Let's make some arrangement to be able to install any module from inside Jupyper - there will be no need to use the Terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can install any module from Jupyter by running a line such as:\n",
    "!{sys.executable} -m pip install module_name\n",
    "\n",
    "We'll need the NLTK module (NLTK stands for Natural Language ToolKit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the NLTK module we'll use a sentence tokenizer 'punkt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/corrine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from pprint import pprint #pretty printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we'll use an electronic archive of books from Project Gutenberg. In particular, we'll use \"Alice in Wonderland\" by Lewis Carrol. Note our corpus will be just one file called carroll-alice.txt (it's in .txt format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/corrine/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "\"[Alice's Adventures in Wonderland b\"\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg') \n",
    "from nltk.corpus import gutenberg \n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt') \n",
    "pprint(alice[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the Alice corpus by sentence by using a sentence tokenizer from the NLTK module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences in alice: 1625\n"
     ]
    }
   ],
   "source": [
    "alice_sentences = nltk.sent_tokenize(text=alice)\n",
    "print('\\nTotal sentences in alice:', len(alice_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first sentence in the Alice corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First sentence in alice: [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.\n"
     ]
    }
   ],
   "source": [
    "print('\\nFirst sentence in alice:', alice_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the second sentence look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second sentence in alice: Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n"
     ]
    }
   ],
   "source": [
    "print('\\nSecond sentence in alice:', alice_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some tokenization by words now. We'll do it on a sentence below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the races\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "print(words)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize by punctuation rules now. Do you see any difference between this tokenization the previous one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of stopwords (\"it's\", \"is\", \"the\", etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourself', \"needn't\", \"don't\", \"you'll\", 'into', 'don', 'while', 'yourselves', 'they', 'then', \"wouldn't\", 'during', 'ain', \"shouldn't\", \"shan't\", 'up', 'i', 'won', 'before', 'where', 'some', 'can', 've', 'for', 'does', 'his', 'after', 'be', 'own', \"didn't\", 'haven', \"weren't\", 'were', 'to', 'wouldn', 'each', 'how', 'have', 'ourselves', 'couldn', 'been', 'about', 'as', 'o', 'we', 'he', 'at', 'it', 'nor', 'which', 'both', \"won't\", 'other', 'doing', 'now', 'will', 'more', 'yours', 'once', 'from', \"wasn't\", \"isn't\", 'until', 'themselves', 'because', 'only', 'any', 'ours', 'y', 'why', 's', 'mustn', 'by', 'few', 'himself', 'are', 'again', 'my', 'that', 'their', 'very', 'll', 'hadn', 'just', 'above', 'didn', 'against', 'those', 'was', 'same', 'needn', 'if', 'ma', 'hasn', 'not', \"you'd\", 'd', \"should've\", 'she', 'down', 'an', 'who', 'than', 'doesn', 'in', 'most', 'had', 'this', 'being', 'off', 'a', 'did', 'no', 'her', \"you're\", 'am', 'all', 'having', 'myself', 'through', 'me', \"you've\", \"doesn't\", \"haven't\", 'over', 'isn', \"hasn't\", 'him', 'these', 't', 'our', 'or', 'weren', 'between', 'too', \"mightn't\", \"mustn't\", 'its', 'is', \"hadn't\", 'wasn', 'of', \"aren't\", 're', 'out', 'such', 'herself', 'when', \"it's\", 'there', 'aren', 'on', 'further', 'below', 'shan', 'hers', 'you', 'your', \"couldn't\", 'with', 'theirs', 'has', 'do', \"that'll\", 'so', 'what', 'them', \"she's\", 'and', 'but', 'should', 'under', 'mightn', 'the', 'm', 'itself', 'whom', 'shouldn', 'here'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the tokenized sentence before and after removing the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n",
      "Filterd Sentence (without stopwords): ['The', 'brown', 'fox', 'quick', 'win', 'races']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens=[]\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "        \n",
    "print(\"Tokenized Sentence:\",words)\n",
    "print(\"Filterd Sentence (without stopwords):\",filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization. Let's stem the sentence first - what changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['The', 'brown', 'fox', 'quick', 'win', 'races']\n",
      "Stemmed Sentence: ['the', 'brown', 'fox', 'quick', 'win', 'race']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_tokens=[]\n",
    "for w in filtered_tokens:\n",
    "    stemmed_tokens.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_tokens)\n",
    "print(\"Stemmed Sentence:\",stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare stemming vs. lemmatization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: run\n",
      "Stemmed Word: run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "word = \"running\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/corrine/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/corrine/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('brown', 'JJ'),\n",
       " ('fox', 'NN'),\n",
       " ('quick', 'JJ'),\n",
       " ('win', 'NN'),\n",
       " ('race', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.pos_tag(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look up the tags here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize the corpus about \"blue skys and blue cheese\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['the sky is blue',\n",
    "          'sky is blue and sky is beautiful', \n",
    "          'the beautiful sky is so blue',\n",
    "          'i love blue cheese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use built-in vectorizers from SciLearn module for machine learning. We'll use bag-of-words representation first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_BOW = CountVectorizer(max_features=1000)\n",
    "BOW_matrix = vectorizer_BOW.fit_transform(corpus).toarray()\n",
    "print(BOW_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do feature extraction (vectorization) using the TF-IDF approach. Note, the results can be slightly different depending on the options you use. See documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         1.         0.         1.22314355 0.\n",
      "  1.22314355 0.         1.51082562]\n",
      " [1.91629073 1.51082562 1.         0.         2.4462871  0.\n",
      "  2.4462871  0.         0.        ]\n",
      " [0.         1.51082562 1.         0.         1.22314355 0.\n",
      "  1.22314355 1.91629073 1.51082562]\n",
      " [0.         0.         1.         1.91629073 0.         1.91629073\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer_TF_IDF = TfidfVectorizer(max_df = 1.0, min_df = 1, norm = None, smooth_idf=True)\n",
    "TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).todense()\n",
    "print(TF_IDF_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the IDF weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.51082562 1.         1.91629073 1.22314355 1.91629073\n",
      " 1.22314355 1.91629073 1.51082562]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_TF_IDF.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to normalize the TF-IDF matrix, i.e. restrict all entries to be between 0 and 1. Some text mining models require normalized matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.39921021 0.         0.48829139 0.\n",
      "  0.48829139 0.         0.60313701]\n",
      " [0.44051607 0.34730793 0.22987956 0.         0.5623514  0.\n",
      "  0.5623514  0.         0.        ]\n",
      " [0.         0.43202578 0.28595344 0.         0.3497621  0.\n",
      "  0.3497621  0.54796992 0.43202578]\n",
      " [0.         0.         0.34618161 0.66338461 0.         0.66338461\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_TF_IDF = TfidfVectorizer(max_df = 1.0, min_df = 1, norm = 'l2', smooth_idf=True)\n",
    "TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).todense()\n",
    "print(TF_IDF_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE: You are given a new small corpus (see below). In Excel, compute the TF-IDF matrix (do not use normalization). Upload your excel file to the Blackboard at the end of class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_exercise = ['python is great for text mining',\n",
    "          'anyone can learn python and do text mining', \n",
    "          'python can go without eating for days',\n",
    "          'python can be a great pet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.51082562 1.         1.91629073 1.22314355 1.91629073\n",
      " 1.22314355 1.91629073 1.51082562]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_BOW = CountVectorizer(max_features=1000)\n",
    "BOW_matrix = vectorizer_BOW.fit_transform(corpus_exercise).toarray()\n",
    "vectorizer_TF_IDF = TfidfVectorizer(max_df = 1.0, min_df = 1, norm = None, smooth_idf=True)\n",
    "TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).todense()\n",
    "print(vectorizer_TF_IDF.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0]\n",
      " [1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0]\n",
      " [0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1]\n",
      " [0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(BOW_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'write' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-382c3614261b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTF_IDF_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'write' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
