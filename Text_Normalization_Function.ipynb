{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT NORMALIZATION FUNCTION\n",
    "\n",
    "Below, we'll define a number of functions that perform various text \"cleaning\" jobs. After each function is defined, you can test that function by running it on a test sentence (object called test_text). At the end of the notebook, we combine all those function into one function called normolize_corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the sentence below for testing the functions. It has punctuations signs, numbers, HTML markups, and other things to take care of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get modules ready and available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html.parser\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/ae/4b752c60868d26d6d14e89882ade7204fd73543e1bde64b6e9b01c1d9856/html-parser-0.2.tar.gz\n",
      "Collecting ply (from html.parser)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/58/35da89ee790598a0700ea49b2a66594140f44dec458c07e8e3d4979137fc/ply-3.11-py2.py3-none-any.whl (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 47kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: html.parser\n",
      "  Building wheel for html.parser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/f5/5e/9f/dbce0d6a89f44b3f30fba0a9b1b24a288882ea2e235e515d7b\n",
      "Successfully built html.parser\n",
      "Installing collected packages: ply, html.parser\n",
      "Successfully installed html.parser ply-3.11\n",
      "Collecting pattern3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/0c/def740f1aaa8c7e3a8c57779187837478f0942eb00b33d4f96246ee63143/pattern3-3.0.0.tar.gz (23.7MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7MB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Users/corrine/miniconda3/lib/python3.7/site-packages (from pattern3) (4.8.1)\n",
      "Collecting cherrypy (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/50/da443d5e52de445462d0c3852549d7fa2e226375a2cb4e9464545cacddb6/CherryPy-18.4.0-py2.py3-none-any.whl (418kB)\n",
      "\u001b[K     |████████████████████████████████| 419kB 14.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docx (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/8e/5a01644697b03016de339ef444cfff28367f92984dc74eddaab1ed60eada/docx-0.2.4.tar.gz (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting feedparser (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 10.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pdfminer3k (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/87/cee0aa24f95c287020df7e3936cb51d32b34b05b430759bac15f89ea5ac2/pdfminer3k-1.3.1.tar.gz (4.1MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1MB 188kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting simplejson (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/24/c35fb1c1c315fc0fffe61ea00d3f88e85469004713dab488dee4f35b0aff/simplejson-3.16.0.tar.gz (81kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pdfminer.six (from pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bd/54795d324f7a44c7189f65bd98d0f2995473e8b27c4b60fd69738b1b92c2/pdfminer.six-20191020-py2.py3-none-any.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 4.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>=1.2 in /Users/corrine/miniconda3/lib/python3.7/site-packages (from beautifulsoup4->pattern3) (1.9.4)\n",
      "Collecting portend>=2.1.1 (from cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/79/eee70a512bffe5ceb5008f8e3326581948f50ca393c3bcb4d557e4818bd1/portend-2.6-py2.py3-none-any.whl\n",
      "Collecting zc.lockfile (from cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/2a/268389776288f0f26c7272c70c36c96dcc0bdb88ab6216ea18e19df1fadd/zc.lockfile-2.0-py2.py3-none-any.whl\n",
      "Collecting cheroot>=8.2.1 (from cherrypy->pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/be/51b1517c6dbf3851d44b36ff08a6e1012464149f89f74c46b29d2f76545e/cheroot-8.2.1-py2.py3-none-any.whl (79kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 3.3MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: more-itertools in /Users/corrine/miniconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (7.2.0)\n",
      "Collecting lxml (from docx->pattern3)\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/9f/6cda4672d3ad1aa4cf818ab8401a763787efba751c88aaf4b38fc8f923bb/lxml-4.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting Pillow>=2.0 (from docx->pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/28/2c72ba965b52884a0bd71e419761fc162763dc2e5d9bec2f3b1949f7272a/Pillow-6.2.1-cp37-cp37m-macosx_10_6_intel.whl (3.9MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9MB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest>=2.0 (from pdfminer3k->pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/16/f6dec5178f5f4141e80dfc4812a9aba88f5f29ca881f174ab1851181d016/pytest-5.2.2-py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 3.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ply>=3.4 in /Users/corrine/miniconda3/lib/python3.7/site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /Users/corrine/miniconda3/lib/python3.7/site-packages (from pdfminer.six->pattern3) (3.0.4)\n",
      "Collecting pycryptodome (from pdfminer.six->pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/70/a1dbdda59273076cd52b569793d7a952d542f6ee98ff07c7adfd8f428e9d/pycryptodome-3.9.1-cp37-cp37m-macosx_10_6_intel.whl (10.1MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1MB 40kB/s eta 0:00:01     |██                              | 604kB 7.7MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/corrine/miniconda3/lib/python3.7/site-packages (from pdfminer.six->pattern3) (1.12.0)\n",
      "Collecting sortedcontainers (from pdfminer.six->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
      "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/12/4c97c44e5c9d111649e363353a4ca3ece9c6cc04b11cc48540f26e42d7b9/tempora-1.14.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /Users/corrine/miniconda3/lib/python3.7/site-packages (from zc.lockfile->cherrypy->pattern3) (41.0.1)\n",
      "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/a4/3e7366d0f5e75dcad7be88524c8cbd0f3a9fb1db243269550981740c57fe/jaraco.functools-2.0-py2.py3-none-any.whl\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest>=2.0->pdfminer3k->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/c7/48439f7d5fd6bddb4c04b850bb862b42e3e2b98570040dfaf68aedd8114b/pluggy-0.13.0-py2.py3-none-any.whl\n",
      "Collecting atomicwrites>=1.0 (from pytest>=2.0->pdfminer3k->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/52/90/6155aa926f43f2b2a22b01be7241be3bfd1ceaf7d0b3267213e8127d41f4/atomicwrites-1.3.0-py2.py3-none-any.whl\n",
      "Collecting packaging (from pytest>=2.0->pdfminer3k->pattern3)\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/94/9672c2d4b126e74c4496c6b3c58a8b51d6419267be9e70660ba23374c875/packaging-19.2-py2.py3-none-any.whl\n",
      "Collecting py>=1.5.0 (from pytest>=2.0->pdfminer3k->pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/bc/394ad449851729244a97857ee14d7cba61ddb268dce3db538ba2f2ba1f0f/py-1.8.0-py2.py3-none-any.whl (83kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 13.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /Users/corrine/miniconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /Users/corrine/miniconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.23)\n",
      "Requirement already satisfied: wcwidth in /Users/corrine/miniconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.1.7)\n",
      "Requirement already satisfied: pytz in /Users/corrine/miniconda3/lib/python3.7/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2019.3)\n",
      "Collecting pyparsing>=2.0.2 (from packaging->pytest>=2.0->pdfminer3k->pattern3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/7828cf9e71ce8fbd43c1e502f3fdd0498f069fcf9d1c268205ce278ae201/pyparsing-2.4.4-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/corrine/miniconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=2.0->pdfminer3k->pattern3) (0.6.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: pattern3, docx, feedparser, pdfminer3k, simplejson\n",
      "  Building wheel for pattern3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/5c/28/a0/1c9344224e87fcdd0efa74c75baaf707ac3f95211056da5889\n",
      "  Building wheel for docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/cc/8d/09/563edfd874a35c0c7ed129b6c4fa890efa4c26458bdec6ffc1\n",
      "  Building wheel for feedparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "  Building wheel for pdfminer3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/ca/4f/a7/cb601b4fb257d2321ac668b7c6e269176780bd0283eda855d2\n",
      "  Building wheel for simplejson (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/corrine/Library/Caches/pip/wheels/5d/1a/1e/0350bb3df3e74215cd91325344cc86c2c691f5306eb4d22c77\n",
      "Successfully built pattern3 docx feedparser pdfminer3k simplejson\n",
      "Installing collected packages: jaraco.functools, tempora, portend, zc.lockfile, cheroot, cherrypy, lxml, Pillow, docx, feedparser, pluggy, atomicwrites, pyparsing, packaging, py, pytest, pdfminer3k, simplejson, pycryptodome, sortedcontainers, pdfminer.six, pattern3\n",
      "Successfully installed Pillow-6.2.1 atomicwrites-1.3.0 cheroot-8.2.1 cherrypy-18.4.0 docx-0.2.4 feedparser-5.2.1 jaraco.functools-2.0 lxml-4.4.1 packaging-19.2 pattern3-3.0.0 pdfminer.six-20191020 pdfminer3k-1.3.1 pluggy-0.13.0 portend-2.6 py-1.8.0 pycryptodome-3.9.1 pyparsing-2.4.4 pytest-5.2.2 simplejson-3.16.0 sortedcontainers-2.1.0 tempora-1.14.1 zc.lockfile-2.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aa7bb81087a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install html.parser\n",
    "import html.parser\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "!{sys.executable} -m pip install pattern3\n",
    "import pattern3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "!{sys.executable} -m pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for lemmatization and HTML parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "html_parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the list of word contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the list of stopwords from NLTK and amend it by adding more stopwords to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/corrine/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also','would']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text into word tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/corrine/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", tokenize_text(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    " \n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", expand_contractions(test_text,contraction_mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate text tokens with Part-Of-Speach tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/corrine/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/corrine/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "def pos_tag_text(text_tokens):\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None  \n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", pos_tag_text(tokenize_text(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize text based on Part-Of-Speach (POS) tags: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    " \n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", lemmatize_text(tokenize_text(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters, such as punctuation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text \n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", remove_special_characters(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", remove_stopwords(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all non-text characters (numbers, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n"
     ]
    }
   ],
   "source": [
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", keep_text_characters(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up HTML markups: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n"
     ]
    }
   ],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "    \n",
    "def strip_html(text):\n",
    "    html_stripper = MLStripper()\n",
    "    html_stripper.feed(text)\n",
    "    return html_stripper.get_data()\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", strip_html(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing accents from characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
     ]
    }
   ],
   "source": [
    "def normalize_accented_characters(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n",
    "    return text\n",
    "\n",
    "print(\"Original:  \", test_text)\n",
    "print(\"Processed: \", normalize_accented_characters(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all functions together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, only_text_chars=True):\n",
    "    normalized_corpus = []  \n",
    "    for index, text in enumerate(corpus):\n",
    "        text = normalize_accented_characters(text)\n",
    "        text = html.unescape(text)\n",
    "        text = strip_html(text)\n",
    "        text = expand_contractions(text, contraction_mapping)\n",
    "        text = tokenize_text(text)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if only_text_chars:\n",
    "            text = keep_text_characters(text)\n",
    "        #text = tokenize_text(text)\n",
    "        normalized_corpus.append(text)    \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small corpus consisting of 2 test sentences and testing the normalize_corpus function on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   [\"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\", \"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\"] \n",
      "\n",
      "Processed:  ['circus dog plisse skirt jump python large foot long', 'circus dog plisse skirt jump python large foot long']\n"
     ]
    }
   ],
   "source": [
    "test_corpus = [test_text]\n",
    "test_corpus.append(test_text)\n",
    "\n",
    "print(\"Original:  \", test_corpus,\"\\n\")\n",
    "print(\"Processed: \", normalize_corpus(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
